{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b366e782-83bd-4f6b-8a01-2ddd65787ab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from censai.models import Autoencoder\n",
    "from censai.cosmos_utils import decode, preprocess\n",
    "from censai.utils import nullwriter\n",
    "import os\n",
    "from datetime import datetime\n",
    "from argparse import ArgumentParser\n",
    "from censai.galflow import convolve\n",
    "from scipy.signal import tukey\n",
    "from numpy import pi\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)\n",
    "\n",
    "\n",
    "class PolynomialSchedule:\n",
    "    def __init__(self, initial_value, end_value, power, decay_steps):\n",
    "        self.initial_value = initial_value\n",
    "        self.end_value = end_value\n",
    "        self.power = power\n",
    "        self.decay_steps = decay_steps\n",
    "\n",
    "    def __call__(self, step=None):\n",
    "        if step is None:\n",
    "            step = tf.summary.experimental.get_step()\n",
    "        step = min(step, self.decay_steps)\n",
    "        return ((self.initial_value - self.end_value) * (1 - step / self.decay_steps) ** (self.power)) + self.end_value\n",
    "\n",
    "\n",
    "def atoi(text):\n",
    "    return int(text) if text.isdigit() else text\n",
    "\n",
    "\n",
    "def natural_keys(text):\n",
    "    '''\n",
    "    alist.sort(key=natural_keys) sorts in human order\n",
    "    http://nedbatchelder.com/blog/200712/human_sorting.html\n",
    "    '''\n",
    "    return [atoi(c) for c in re.split(r'(\\d+)', text)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ed94a265-b6b6-4354-b90e-e10ab8c030a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.argv = ['']\n",
    "date = datetime.now().strftime(\"%y-%m-%d_%H-%M-%S\")\n",
    "parser = ArgumentParser()\n",
    "parser.add_argument(\"--model_id\", type=str, default=\"None\",\n",
    "                    help=\"Start from this model id checkpoint. None means start from scratch\")\n",
    "parser.add_argument(\"--pixels\", default=128, type=int, help=\"Number of pixels on a side, should be fixed for a given cosmos tfrecord\")\n",
    "parser.add_argument(\"--num_parallel_reads\", default=1, type=int, help=\"TF dataset number of parallel processes loading the data while training\")\n",
    "parser.add_argument(\"--data\", default=\"../data/cosmos_25.2\", help=\"Path to the data root directory, containing tf records files\")\n",
    "\n",
    "# training params\n",
    "parser.add_argument(\"--split\", default=0.8, type=float, help=\"Training split, number in the range [0.5, 1)\")\n",
    "parser.add_argument(\"--test_shards\", default=70, type=int, help=\"Number of shards to keep as a test set. The largest shard index are kept\")\n",
    "parser.add_argument(\"--examples_per_shard\", default=1000, type=int,\n",
    "                    help=\"Number of example on a given COSMO shard. Should match the parameter of cosmo_to_tfrecords with which it was generated\")\n",
    "parser.add_argument(\"-b\", \"--batch_size\", default=100, type=int, required=False, help=\"Number of images in a batch\")\n",
    "parser.add_argument(\"-e\", \"--epochs\", default=1, type=int, help=\"Number of epochs for training\")\n",
    "parser.add_argument(\"--patience\", default=np.inf, type=float, help=\"Number of epoch at which \"\n",
    "                                                            \"training is stop if no improvement have been made\")\n",
    "parser.add_argument(\"--tolerance\", default=0, type=float,\n",
    "                    help=\"Percentage [0-1] of improvement required for patience to reset. The most lenient \"\n",
    "                                                    \"value is 0 (any improvement reset patience)\")\n",
    "parser.add_argument(\"--learning_rate\", default=1e-4, type=float,\n",
    "                    help=\"Initial value of the learning rate\")\n",
    "parser.add_argument(\"--decay_rate\", type=float, default=1,\n",
    "                    help=\"Decay rate of the exponential decay schedule of the learning rate. 1=no decay\")\n",
    "parser.add_argument(\"--decay_steps\", type=int, default=100)\n",
    "parser.add_argument(\"--staircase\", action=\"store_true\", help=\"Learning schedule is a staircase \"\n",
    "                                                             \"function if added to arguments\")\n",
    "parser.add_argument(\"--apodization_alpha\", default=1, type=float,\n",
    "                    help=\"Shape parameter of the Tukey window (Tapered cosine Window),\"\\\n",
    "                    \"representing the fraction of the window inside the cosine tapered region.\"\\\n",
    "                    \"If zero, the Tukey window is equivalent to a rectangular window (no apodization)\"\\\n",
    "                    \"If one, the Tukey window is equivalent to a Hann window.\")\n",
    "parser.add_argument(\"--apodization_factor\", default=1, type=float,\n",
    "                    help=\"Lagrange multiplier of apodization loss\")\n",
    "parser.add_argument(\"--tv_factor\", default=1, type=float,\n",
    "                    help=\"Lagrange multiplier of Total Variation (TV) loss. Penalize high spatial frequency\"\n",
    "                         \"components in the predicted image\")\n",
    "parser.add_argument(\"--l2_bottleneck\", default=1, type=float,\n",
    "                    help=\"Initial value of l2 penalty in bottleneck identity \"\n",
    "                         \"map of encoder/decoder latent representation\")\n",
    "parser.add_argument(\"--l2_bottleneck_decay_steps\", default=1000, type=int,\n",
    "                    help=\"Number of steps until l2 bottleneck penalty factor reaches 0\")\n",
    "parser.add_argument(\"--l2_bottleneck_decay_power\", default=0.2, type=float,\n",
    "                    help=\"Control the shape of the decay of l2_bottlenck schedule (0.5=square root decay, etc.)\")\n",
    "parser.add_argument(\"--skip_strength\", default=1, type=float,\n",
    "                    help=\"Initial value of the multiplicative factor in front of the Unet additive skip between \"\n",
    "                         \"encoder and decoder layers.\")\n",
    "parser.add_argument(\"--skip_strength_decay_steps\", default=1000, type=int,\n",
    "                    help=\"Number of steps until skip_strength reaches 0\")\n",
    "parser.add_argument(\"--skip_strength_decay_power\", default=0.5, type=float,\n",
    "                    help=\"Control the shape of the decay for skip_strength schedule\")\n",
    "\n",
    "\n",
    "# model hyperparameters\n",
    "parser.add_argument(\"--res_layers\", default=7, type=int,\n",
    "                    help=\"Number of downsampling block in encoder (symmetric in decoder\")\n",
    "parser.add_argument(\"--conv_layers_in_res_block\", default=2, type=int,\n",
    "                    help=\"Number of conv layers in a Residual block\")\n",
    "parser.add_argument(\"--filter_scaling\", default=2, type=float,\n",
    "                    help=\"Filters scale by {filter_scaling}^{res_layer_index}, generally number between (1, 2]\")\n",
    "parser.add_argument(\"--filter_init\", default=8, type=int,\n",
    "                    help=\"Number of filters in the first residual block (before last for decoder)\")\n",
    "parser.add_argument(\"--kernel_size\", default=3, type=int,\n",
    "                    help=\"Size of the kernels throughout model\")\n",
    "parser.add_argument(\"--kernel_reg_amp\", default=1e-2, type=float,\n",
    "                    help=\"Amplitude of l2 regularization for kernel weights in the model\")\n",
    "parser.add_argument(\"--bias_reg_amp\", default=1e-2, type=float,\n",
    "                    help=\"Amplitude of l2 regularization for bias variables in the model\")\n",
    "parser.add_argument(\"--relu_alpha\", default=0.1, type=float,\n",
    "                    help=\"Slope of LeakyReLu in the negative plane\")\n",
    "parser.add_argument(\"--resblock_dropout_rate\", default=None, type=float,\n",
    "                    help=\"Number between [0, 1), number of filters to drop at each call. Default is to not use dropout\")\n",
    "parser.add_argument(\"--latent_size\", default=16, type=int,\n",
    "                    help=\"Size of the latent vector space\")\n",
    "parser.add_argument(\"--res_architecture\", default=\"bare\", type=str, \n",
    "                    help=\"Name of the Resnet Block architecture. Options are \"\\\n",
    "                         \"'bare', 'original', 'bn_after_addition', \"\\\n",
    "                         \"'relu_before_addition', 'relu_only_pre_activation', \"\\\n",
    "                         \"'full_pre_activation', 'full_pre_activation_rescale'\")\n",
    "\n",
    "# logs\n",
    "parser.add_argument(\"--logdir\", default=\"None\",\n",
    "                    help=\"Path of logs directory. Default if None, no logs recorded\")\n",
    "parser.add_argument(\"--model_dir\", default=\"None\",\n",
    "                    help=\"Path to the directory where to save models checkpoints\")\n",
    "parser.add_argument(\"--checkpoints\", default=10, type=int,\n",
    "                    help=\"Save a checkpoint of the models each {%} iteration\")\n",
    "parser.add_argument(\"--max_to_keep\", default=3, type=int,\n",
    "                    help=\"Max model checkpoint to keep\")\n",
    "parser.add_argument(\"--logname\", default=\"cosmosAE_\" + date,\n",
    "                    help=\"Name of the logs, default is the local date + time\")\n",
    "\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b5e74d4a-a0ac-4d8c-b5cf-4ca25d201c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_file = os.path.join(os.getenv(\"SLURM_TMPDIR\"), \"cache\") # temporary location of dataset\n",
    "\n",
    "filenames = os.listdir(args.data)\n",
    "filenames.sort(key=natural_keys)\n",
    "# keep the n last files as a test set\n",
    "if args.test_shards != 0:\n",
    "    filenames = filenames[:-args.test_shards]\n",
    "filenames = [os.path.join(args.data, file) for file in filenames]\n",
    "train_size = len(filenames) * args.examples_per_shard  # estimate the length of the dataset\n",
    "dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=args.num_parallel_reads)\n",
    "dataset = dataset.map(decode)\n",
    "dataset = dataset.map(preprocess)\n",
    "dataset = dataset.shuffle(buffer_size=args.examples_per_shard)  # shuffle images inside a shard\n",
    "train_dataset = dataset.take(int(train_size * args.split))\n",
    "test_dataset = dataset.skip(int(train_size * (1 - args.split)))\n",
    "train_dataset = train_dataset.batch(args.batch_size, drop_remainder=False)\n",
    "test_dataset = test_dataset.batch(args.batch_size, drop_remainder=True)\n",
    "train_dataset = train_dataset.enumerate()\n",
    "train_dataset = train_dataset.cache(cache_file)\n",
    "test_dataset = test_dataset.cache(cache_file)\n",
    "train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "learning_rate_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=args.learning_rate,\n",
    "    decay_rate=args.decay_rate,\n",
    "    decay_steps=args.decay_steps,\n",
    "    staircase=args.staircase\n",
    ")\n",
    "skip_strength_schedule = PolynomialSchedule(\n",
    "    initial_value=args.skip_strength,\n",
    "    end_value=0.,\n",
    "    power=args.skip_strength_decay_power,\n",
    "    decay_steps=args.skip_strength_decay_steps\n",
    ")\n",
    "l2_bottleneck_schedule = PolynomialSchedule(\n",
    "    initial_value=args.l2_bottleneck,\n",
    "    end_value=0.,\n",
    "    power=args.l2_bottleneck_decay_power,\n",
    "    decay_steps=args.l2_bottleneck_decay_steps\n",
    ")\n",
    "\n",
    "optim = tf.optimizers.Adam(learning_rate=learning_rate_schedule)\n",
    "AE = Autoencoder(\n",
    "    pixels=args.pixels,\n",
    "    res_layers=args.res_layers,\n",
    "    conv_layers_in_resblock=args.conv_layers_in_res_block,\n",
    "    filter_scaling=args.filter_scaling,\n",
    "    filter_init=args.filter_init,\n",
    "    kernel_size=args.kernel_size,\n",
    "    res_architecture=args.res_architecture,\n",
    "    kernel_reg_amp=args.kernel_reg_amp,\n",
    "    bias_reg_amp=args.bias_reg_amp,\n",
    "    alpha=args.relu_alpha,\n",
    "    resblock_dropout_rate=args.resblock_dropout_rate,\n",
    "    latent_size=args.latent_size\n",
    ")\n",
    "\n",
    "# setup tensorboard writer (nullwriter in case we do not want to sync)\n",
    "if args.model_id.lower() != \"none\":\n",
    "    logname = args.model_id\n",
    "else:\n",
    "    logname = args.logname\n",
    "if args.logdir.lower() != \"none\":\n",
    "    logdir = os.path.join(args.logdir, logname)\n",
    "    traindir = os.path.join(logdir, \"train\")\n",
    "    testdir = os.path.join(logdir, \"test\")\n",
    "    if not os.path.isdir(logdir):\n",
    "        os.mkdir(logdir)\n",
    "    if not os.path.isdir(traindir):\n",
    "        os.mkdir(traindir)\n",
    "    if not os.path.isdir(testdir):\n",
    "        os.mkdir(testdir)\n",
    "    train_writer = tf.summary.create_file_writer(traindir)\n",
    "    test_writer = tf.summary.create_file_writer(testdir)\n",
    "else:\n",
    "    test_writer = nullwriter()\n",
    "    train_writer = nullwriter()\n",
    "if args.model_dir.lower() != \"none\":\n",
    "    models_dir = os.path.join(args.model_dir, logname)\n",
    "    if not os.path.isdir(models_dir):\n",
    "        os.mkdir(models_dir)\n",
    "    encoder_checkpoints_dir = os.path.join(models_dir, \"encoder_checkpoints\")\n",
    "    if not os.path.isdir(encoder_checkpoints_dir):\n",
    "        os.mkdir(encoder_checkpoints_dir)\n",
    "    decoder_checkpoints_dir = os.path.join(models_dir, \"decoder_checkpoints\")\n",
    "    if not os.path.isdir(decoder_checkpoints_dir):\n",
    "        os.mkdir(decoder_checkpoints_dir)\n",
    "    encoder_ckpt = tf.train.Checkpoint(step=tf.Variable(1), optimizer=optim, net=AE.encoder)\n",
    "    encoder_checkpoint_manager = tf.train.CheckpointManager(encoder_ckpt, encoder_checkpoints_dir, max_to_keep=3)\n",
    "    decoder_ckpt = tf.train.Checkpoint(step=tf.Variable(1), optimizer=optim, net=AE.decoder)\n",
    "    decoder_checkpoint_manager = tf.train.CheckpointManager(decoder_ckpt, decoder_checkpoints_dir, max_to_keep=3)\n",
    "    save_checkpoint = True\n",
    "    if args.model_id.lower() != \"none\":\n",
    "        encoder_checkpoint_manager.checkpoint.restore(encoder_checkpoint_manager.latest_checkpoint)\n",
    "        decoder_checkpoint_manager.checkpoint.restore(decoder_checkpoint_manager.latest_checkpoint)\n",
    "else:\n",
    "    save_checkpoint = False\n",
    "\n",
    "epoch_loss = tf.metrics.Mean()\n",
    "test_loss = tf.metrics.Mean()\n",
    "best_loss = np.inf\n",
    "patience = args.patience\n",
    "step = 0\n",
    "end = 0\n",
    "time_per_step = tf.metrics.Mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "56f5c89e-a5d7-4408-a5ee-4d13b1956263",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 32, Time 0.1514   \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:04<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-d20de0cd87f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m                 ))\n\u001b[1;32m     23\u001b[0m                 \u001b[0mcost\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Add layer specific regularizer losses (L2 in definitions)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0mgradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m             \u001b[0;31m# clipped_gradient = [tf.clip_by_value(grad, -10, 10) for grad in gradient]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# backprop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/environments/censai3.8/lib/python3.8/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1078\u001b[0m                           for x in nest.flatten(output_gradients)]\n\u001b[1;32m   1079\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1080\u001b[0;31m     flat_grad = imperative_grad.imperative_grad(\n\u001b[0m\u001b[1;32m   1081\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1082\u001b[0m         \u001b[0mflat_targets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/environments/censai3.8/lib/python3.8/site-packages/tensorflow/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     69\u001b[0m         \"Unknown value for unconnected_gradients: %r\" % unconnected_gradients)\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m   return pywrap_tfe.TFE_Py_TapeGradient(\n\u001b[0m\u001b[1;32m     72\u001b[0m       \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m       \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/environments/censai3.8/lib/python3.8/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[1;32m    160\u001b[0m       \u001b[0mgradient_name_scope\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mforward_pass_name_scope\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient_name_scope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/environments/censai3.8/lib/python3.8/site-packages/tensorflow/python/ops/nn_grad.py\u001b[0m in \u001b[0;36m_Conv2DGrad\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m    585\u001b[0m   \u001b[0;31m# in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    586\u001b[0m   return [\n\u001b[0;32m--> 587\u001b[0;31m       gen_nn_ops.conv2d_backprop_input(\n\u001b[0m\u001b[1;32m    588\u001b[0m           \u001b[0mshape_0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m           \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/environments/censai3.8/lib/python3.8/site-packages/tensorflow/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d_backprop_input\u001b[0;34m(input_sizes, filter, out_backprop, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name)\u001b[0m\n\u001b[1;32m   1238\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1239\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1240\u001b[0;31m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[1;32m   1241\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Conv2DBackpropInput\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_backprop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1242\u001b[0m         \u001b[0;34m\"strides\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"use_cudnn_on_gpu\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"padding\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in tqdm(range(args.epochs)):\n",
    "    epoch_loss.reset_states()\n",
    "    test_loss.reset_states()\n",
    "    with train_writer.as_default():\n",
    "        for batch, (X, PSF, PS) in train_dataset:\n",
    "            if step != 0:\n",
    "                time_per_step.update_state([time.time() - start])\n",
    "            start = time.time()\n",
    "            sys.stdout.write(\"Step: %d, Time %.4f   \\r\" % (step, time_per_step.result()) )\n",
    "            sys.stdout.flush()\n",
    "            with tf.GradientTape() as tape:\n",
    "                tape.watch(AE.trainable_variables)\n",
    "                cost = tf.reduce_mean(AE.training_cost_function(\n",
    "                    x=X,\n",
    "                    psf=PSF,\n",
    "                    ps=PS,\n",
    "                    skip_strength=skip_strength_schedule(step),\n",
    "                    l2_bottleneck=l2_bottleneck_schedule(step),\n",
    "                    apodization_alpha=args.apodization_alpha,\n",
    "                    apodization_factor=args.apodization_factor,\n",
    "                    tv_factor=args.tv_factor\n",
    "                ))\n",
    "                cost += tf.reduce_sum(AE.losses)  # Add layer specific regularizer losses (L2 in definitions)\n",
    "            gradient = tape.gradient(cost, AE.trainable_variables)\n",
    "            # clipped_gradient = [tf.clip_by_value(grad, -10, 10) for grad in gradient]\n",
    "            optim.apply_gradients(zip(gradient, AE.trainable_variables)) # backprop\n",
    "\n",
    "            #========== Summary and logs ==========\n",
    "            epoch_loss.update_state([cost])\n",
    "            tf.summary.scalar(\"MSE\", cost, step=step)\n",
    "            step += 1\n",
    "        tf.summary.scalar(\"Learning Rate\", optim.lr(step), step=step)\n",
    "    with test_writer.as_default():\n",
    "        for X, PSF, PS in test_dataset:\n",
    "            time_per_step.update_state([time.time() - start])\n",
    "            start = time.time()\n",
    "            sys.stdout.write(\"Test, Time %.4f   \\r\" % (time_per_step.result()) )\n",
    "            sys.stdout.flush()\n",
    "            test_cost = tf.reduce_mean(AE.training_cost_function(\n",
    "                x=X,\n",
    "                psf=PSF,\n",
    "                ps=PS,\n",
    "                skip_strength=skip_strength_schedule(step),\n",
    "                l2_bottleneck=l2_bottleneck_schedule(step),\n",
    "                apodization_alpha=args.apodization_alpha,\n",
    "                apodization_factor=args.apodization_factor,\n",
    "                tv_factor=args.tv_factor\n",
    "            ))\n",
    "            test_loss.update_state([test_cost])\n",
    "    tf.summary.scalar(\"MSE\", test_loss.result(), step=step)\n",
    "    print(f\"epoch {epoch} | train loss {epoch_loss.result().numpy():.3e} | val loss {test_loss.result().numpy():.3e} \"\n",
    "          f\"| learning rate {optim.lr(step).numpy():.2e}\")\n",
    "    if epoch_loss.result() < (1 - args.tolerance) * best_loss:\n",
    "        best_loss = epoch_loss.result()\n",
    "        patience = args.patience\n",
    "    else:\n",
    "        patience -= 1\n",
    "    if save_checkpoint:\n",
    "        encoder_checkpoint_manager.checkpoint.step.assign_add(1) # a bit of a hack\n",
    "        decoder_checkpoint_manager.checkpoint.step.assign_add(1)\n",
    "        if epoch % args.checkpoints == 0 or patience == 0 or epoch == args.epochs - 1:\n",
    "            encoder_checkpoint_manager.save()\n",
    "            decoder_checkpoint_manager.save()\n",
    "            print(\"Saved checkpoint for step {}: {}\".format(int(encoder_checkpoint_manager.checkpoint.step),\n",
    "                                                            encoder_checkpoint_manager.latest_checkpoint))\n",
    "    if patience == 0:\n",
    "        print(\"Reached patience\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "4087ce6e-54c9-4504-a431-9a1916138f21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96.57302989075099"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l2_bottleneck_schedule(step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "cdee9e4b-bf67-4b4a-aa76-b55f2fe835ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=9004.25>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_mean(AE.training_cost_function(\n",
    "                    x=X,\n",
    "                    psf=PSF,\n",
    "                    ps=PS,\n",
    "                    skip_strength=skip_strength_schedule(step),\n",
    "                    l2_bottleneck=l2_bottleneck_schedule(step),\n",
    "                    apodization_alpha=args.apodization_alpha,\n",
    "                    apodization_factor=args.apodization_factor,\n",
    "                    tv_factor=args.tv_factor\n",
    "                ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "da9604bb-cb9d-47b9-94d9-d36ebe96abcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=X\n",
    "psf=PSF\n",
    "ps=PS\n",
    "skip_strength=skip_strength_schedule(step)\n",
    "l2_bottleneck=l2_bottleneck_schedule(step)\n",
    "apodization_alpha=args.apodization_alpha\n",
    "apodization_factor=args.apodization_factor\n",
    "tv_factor=args.tv_factor\n",
    "self = AE\n",
    "\n",
    "\n",
    "input_shape = x.shape\n",
    "psf_image = tf.signal.irfft2d(tf.cast(psf[..., 0], tf.complex64))[..., tf.newaxis]\n",
    "# Roll the image to undo the fftshift, assuming x1 zero padding and x2 subsampling\n",
    "psf_image = tf.roll(psf_image, shift=[input_shape[1], input_shape[2]], axis=[1, 2])\n",
    "psf_image = tf.image.resize_with_crop_or_pad(psf_image, input_shape[1], input_shape[2])\n",
    "# x = self.link_function(x)\n",
    "# psf_image = self.link_function(psf_image)\n",
    "\n",
    "# x = tf.concat([x, psf_image], axis=-1)\n",
    "# print(x)\n",
    "# z, skips = self.encoder.call_with_skip_connections(x)\n",
    "# print(z)\n",
    "# x_pred, bottleneck_l2_cost = self.decoder.call_with_skip_connections(z, skips, skip_strength, l2_bottleneck)\n",
    "# print(x_pred)\n",
    "# x_pred = self.inverse_link_function(x_pred)\n",
    "# x_pred = convolve(x_pred, tf.cast(psf[..., 0], tf.complex64), zero_padding_factor=1) # we already padded psf with noise in data preprocessing\n",
    "\n",
    "# # We apply an optional apodization of the output before taking the\n",
    "# if apodization_alpha > 0 and apodization_factor > 0:\n",
    "#     nx = x_pred.shape[1]\n",
    "#     alpha = 2 * apodization_alpha / nx\n",
    "#     # Create a tukey window\n",
    "#     w = tukey(nx, alpha)\n",
    "#     w = np.outer(w, w).reshape((1, nx, nx, 1)).astype('float32')\n",
    "#     # And penalize non zero things at the border\n",
    "#     apo_loss = apodization_factor * tf.reduce_mean(tf.reduce_sum(((1. - w) * x_pred) ** 2, axis=[1, 2, 3]))\n",
    "# else:  # rectangular window\n",
    "#     w = 1.0\n",
    "#     apo_loss = 0.\n",
    "\n",
    "# # We apply the window\n",
    "# x_pred = x_pred * w\n",
    "\n",
    "# # apply tv loss\n",
    "# if tv_factor > 0:\n",
    "#     tv_loss = tv_factor * tf.image.total_variation(x_pred)\n",
    "#     # Smoothed Isotropic TV:\n",
    "#     # im_dx, im_dy = tf.image.image_gradients(x_pred)\n",
    "#     # tv_loss = tv_factor * tf.reduce_sum(tf.sqrt(im_dx**2 + im_dy**2 + 1e-6), axis=[1,2,3])\n",
    "# else:\n",
    "#     tv_loss = 0.\n",
    "\n",
    "# x = tf.signal.rfft2d(x[..., 0])\n",
    "# x_pred = tf.signal.rfft2d(x_pred[..., 0])\n",
    "\n",
    "# # added a safety net in the division, even if tfrecords were generated to ensure\n",
    "# chi_squared = 0.5 * tf.reduce_mean(tf.abs((x - x_pred)**2 / tf.complex(tf.exp(ps)[..., 0] + 1e-8, 0.) / (2 * pi) ** 2), axis=[1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ff93cc34-f1c0-4f52-83b4-1fedc804fda2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(self.link_function(psf_image).numpy().ravel(), bins=100);\n",
    "# plt.xscale(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c26a44b-62a9-43f2-9cbd-6bfce8496855",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# AE.decoder.trainable_weights\n",
    "for batch, (X, PSF, PS) in train_dataset:\n",
    "    plt.hist(X.numpy().ravel(), bins=100)\n",
    "    plt.xscale(\"log\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b74cc0-3dc4-4b6d-9496-0f4537e72d3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "censai",
   "language": "python",
   "name": "censai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
