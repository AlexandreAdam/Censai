{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "considered-variance",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from censai import PhysicalModelv2, RIMSourceUnetv2, PowerSpectrum\n",
    "from censai.models import UnetModelv2, RayTracer\n",
    "from censai.utils import nullwriter, rim_residual_plot as residual_plot, plot_to_image\n",
    "from censai.data.lenses_tng_v3 import decode_train, decode_physical_model_info\n",
    "import os, glob, re, json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from argparse import Namespace\n",
    "import math, json\n",
    "import matplotlib.pylab as pylab\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 2000)\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "\n",
    "result_dir = os.path.join(os.getenv(\"CENSAI_PATH\"), \"results\")\n",
    "data_path = os.path.join(os.getenv(\"CENSAI_PATH\"), \"data\")\n",
    "models_path = os.path.join(os.getenv(\"CENSAI_PATH\"), \"models\")\n",
    "\n",
    "params = {'legend.fontsize': 'x-large',\n",
    "#           'figure.figsize': (10, 10),\n",
    "         'axes.labelsize': 'x-large',\n",
    "         'axes.titlesize':'x-large',\n",
    "         'xtick.labelsize':'x-large',\n",
    "         'ytick.labelsize':'x-large'}\n",
    "pylab.rcParams.update(params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "automotive-reserve",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10138\n"
     ]
    }
   ],
   "source": [
    "model = \"RIMSource128hstv3_control_002_A0_L3_FLM1.0_211108220845\"\n",
    "\n",
    "checkpoints_dir = os.path.join(os.getenv(\"CENSAI_PATH\"), \"models\", model)\n",
    "with open(os.path.join(checkpoints_dir, \"script_params.json\"), \"r\") as f:\n",
    "    args = json.load(f)\n",
    "args = Namespace(**args)\n",
    "_dataset = os.path.split(args.val_datasets[0])[-1]\n",
    "\n",
    "\n",
    "files = glob.glob(os.path.join(os.getenv(\"CENSAI_PATH\"), \"data\", _dataset, \"*.tfrecords\"))\n",
    "files = tf.data.Dataset.from_tensor_slices(files)\n",
    "dataset = files.interleave(lambda x: tf.data.TFRecordDataset(x, compression_type=\"GZIP\"),\n",
    "                           block_length=1, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "for physical_params in dataset.map(decode_physical_model_info):\n",
    "    break\n",
    "dataset = dataset.map(decode_train)\n",
    "\n",
    "total_items = int(np.loadtxt(os.path.join(os.getenv(\"CENSAI_PATH\"), \"data\", _dataset, \"dataset_size.txt\")))\n",
    "print(total_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "valued-vegetation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x2b131d5a8310>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phys = PhysicalModelv2(\n",
    "    pixels=physical_params[\"pixels\"].numpy(),\n",
    "    kappa_pixels=physical_params[\"kappa pixels\"].numpy(),\n",
    "    src_pixels=physical_params[\"src pixels\"].numpy(),\n",
    "    image_fov=physical_params[\"image fov\"].numpy(),\n",
    "    kappa_fov=physical_params[\"kappa fov\"].numpy(),\n",
    "    src_fov=physical_params[\"source fov\"].numpy(),\n",
    "    method=\"fft\"\n",
    ")\n",
    "\n",
    "unet = UnetModelv2(\n",
    "    filters=args.filters,\n",
    "    filter_scaling=args.filter_scaling,\n",
    "    kernel_size=args.kernel_size,\n",
    "    layers=args.layers,\n",
    "    block_conv_layers=args.block_conv_layers,\n",
    "    strides=args.strides,\n",
    "    bottleneck_kernel_size=args.bottleneck_kernel_size,\n",
    "    resampling_kernel_size=args.resampling_kernel_size,\n",
    "    input_kernel_size=args.input_kernel_size,\n",
    "    gru_kernel_size=args.gru_kernel_size,\n",
    "    upsampling_interpolation=args.upsampling_interpolation,\n",
    "    kernel_l2_amp=args.kernel_l2_amp,\n",
    "    bias_l2_amp=args.bias_l2_amp,\n",
    "    kernel_l1_amp=args.kernel_l1_amp,\n",
    "    bias_l1_amp=args.bias_l1_amp,\n",
    "    activation=args.activation,\n",
    "    initializer=args.initializer,\n",
    "    batch_norm=args.batch_norm,\n",
    "    dropout_rate=args.dropout_rate\n",
    ")\n",
    "rim = RIMSourceUnetv2(\n",
    "    physical_model=phys,\n",
    "    unet=unet,\n",
    "    steps=args.steps,\n",
    "    adam=args.adam,\n",
    "    source_link=args.source_link,\n",
    "    source_init=args.source_init,\n",
    "    flux_lagrange_multiplier=args.flux_lagrange_multiplier\n",
    ")\n",
    "ckpt = tf.train.Checkpoint(net=rim.unet)\n",
    "checkpoint_manager = tf.train.CheckpointManager(ckpt, checkpoints_dir, max_to_keep=args.max_to_keep)\n",
    "checkpoint_manager.checkpoint.restore(checkpoint_manager.latest_checkpoint).expect_partial()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "facial-encounter",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [10:47<00:00, 58.83s/it]\n"
     ]
    }
   ],
   "source": [
    "k_bins = 40\n",
    "# total_items = 10000\n",
    "examples_per_shard = 1000\n",
    "batch_size = 10\n",
    "\n",
    "\n",
    "output_dir = os.path.join(os.getenv(\"CENSAI_PATH\"), \"results\", model + \"_\" + _dataset)\n",
    "if not os.path.isdir(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "ps_lens = PowerSpectrum(bins=k_bins, pixels=physical_params[\"pixels\"].numpy())\n",
    "ps_x = PowerSpectrum(bins=k_bins,  pixels=physical_params[\"src pixels\"].numpy())\n",
    "\n",
    "shards = total_items // examples_per_shard + 1 * (total_items % examples_per_shard > 0)\n",
    "k = 0\n",
    "for shard in tqdm(range(shards)):\n",
    "    hf = h5py.File(os.path.join(output_dir, f\"predictions_{shard:02d}.h5\"), 'w')\n",
    "    data = dataset.skip(shard * examples_per_shard).take(examples_per_shard).batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    for batch, (lens, source, kappa, noise_rms, psf) in enumerate(data):\n",
    "        source_pred, chi_squared = rim.predict(lens, kappa, noise_rms, psf)\n",
    "        lens_pred = phys.forward(source_pred[-1], kappa, psf)\n",
    "\n",
    "        # remove channel dimension because power spectrum expect only [batch, pixels, pixels] shaped tensor.\n",
    "        _ps_lens = ps_lens.cross_correlation_coefficient(lens[..., 0], lens_pred[..., 0])\n",
    "        _ps_source = ps_x.cross_correlation_coefficient(source[..., 0], source_pred[-1][..., 0])\n",
    "\n",
    "        batch_size = lens.shape[0]\n",
    "        for b in range(batch_size):\n",
    "            g = hf.create_group(f'data_{k:d}')\n",
    "            g.create_dataset(\"lens\",        data=lens[b])\n",
    "            g.create_dataset(\"source\",      data=source[b])\n",
    "            g.create_dataset(\"kappa\",       data=kappa[b])\n",
    "            g.create_dataset(\"lens_pred\",   data=lens_pred[b])\n",
    "            g.create_dataset(\"source_pred\", data=source_pred[:, b])\n",
    "            g.create_dataset(\"chi_squared\", data=chi_squared[:, b])\n",
    "            g.create_dataset(\"ps_lens\",     data=_ps_lens[b])\n",
    "            g.create_dataset(\"ps_source\",   data=_ps_source[b])\n",
    "            k += 1\n",
    "    hf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7c9cf40f-f560-40db-86d1-d69fa58026e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/aadam/scratch/Censai/results/RIMSource128hstv3_control_002_A0_L3_FLM1.0_211108220845_lenses128hst_TNG_rau_200k_control_denoised_validated_val'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebd3274-8150-4eaa-83ab-1a2f3a36e598",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "censai",
   "language": "python",
   "name": "censai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
