{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5d679e-2bb4-410e-a84b-bef211b11c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from censai import PhysicalModel, RIMUnet, RayTracer\n",
    "from censai.models import UnetModel, VAE, VAESecondStage\n",
    "from censai.utils import nullwriter, rim_residual_plot as residual_plot, plot_to_image\n",
    "import os, time, json\n",
    "from datetime import datetime\n",
    "\n",
    "RIM_HPARAMS = [\n",
    "    \"adam\",\n",
    "    \"steps\",\n",
    "    \"kappalog\",\n",
    "    \"kappa_normalize\",\n",
    "    \"kappa_init\",\n",
    "    \"source_init\"\n",
    "]\n",
    "SOURCE_MODEL_HPARAMS = [\n",
    "    \"filters\",\n",
    "    \"filter_scaling\",\n",
    "    \"kernel_size\",\n",
    "    \"layers\",\n",
    "    \"block_conv_layers\",\n",
    "    \"strides\",\n",
    "    \"bottleneck_kernel_size\",\n",
    "    \"bottleneck_filters\",\n",
    "    \"resampling_kernel_size\",\n",
    "    \"gru_kernel_size\",\n",
    "    \"upsampling_interpolation\",\n",
    "    \"kernel_regularizer_amp\",\n",
    "    \"bias_regularizer_amp\",\n",
    "    \"activation\",\n",
    "    \"alpha\",\n",
    "    \"initializer\"\n",
    "]\n",
    "KAPPA_MODEL_HPARAMS = [\n",
    "    \"filters\",\n",
    "    \"filter_scaling\",\n",
    "    \"kernel_size\",\n",
    "    \"layers\",\n",
    "    \"block_conv_layers\",\n",
    "    \"strides\",\n",
    "    \"bottleneck_kernel_size\",\n",
    "    \"bottleneck_filters\",\n",
    "    \"resampling_kernel_size\",\n",
    "    \"gru_kernel_size\",\n",
    "    \"upsampling_interpolation\",\n",
    "    \"kernel_regularizer_amp\",\n",
    "    \"bias_regularizer_amp\",\n",
    "    \"activation\",\n",
    "    \"alpha\",\n",
    "    \"initializer\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2824a8c3-ec31-48e1-8972-330f3b8d76cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    if args.raytracer is not None:\n",
    "        with open(os.path.join(args.raytracer, \"ray_tracer_hparams.json\"), \"r\") as f:\n",
    "            raytracer_hparams = json.load(f)\n",
    "    if args.raytracer is not None:\n",
    "        raytracer = RayTracer(**raytracer_hparams)\n",
    "        # load last checkpoint in the checkpoint directory\n",
    "        checkpoint = tf.train.Checkpoint(net=raytracer)\n",
    "        manager = tf.train.CheckpointManager(checkpoint, directory=args.raytracer, max_to_keep=3)\n",
    "        checkpoint.restore(manager.latest_checkpoint).expect_partial()\n",
    "    else:\n",
    "        raytracer = None\n",
    "\n",
    "    # with tf.device(vae_device):\n",
    "    # =============== kappa vae ========================================\n",
    "    # Load first stage and freeze weights\n",
    "    with open(os.path.join(args.kappa_first_stage_vae, \"model_hparams.json\"), \"r\") as f:\n",
    "        kappa_vae_hparams = json.load(f)\n",
    "    kappa_vae = VAE(**kappa_vae_hparams)\n",
    "    ckpt1 = tf.train.Checkpoint(step=tf.Variable(1), net=kappa_vae)\n",
    "    checkpoint_manager1 = tf.train.CheckpointManager(ckpt1, args.kappa_first_stage_vae, 1)\n",
    "    checkpoint_manager1.checkpoint.restore(checkpoint_manager1.latest_checkpoint).expect_partial()\n",
    "    kappa_vae.trainable = False\n",
    "    kappa_vae.encoder.trainable = False\n",
    "    kappa_vae.decoder.trainable = False\n",
    "\n",
    "    # Setup sampling from second stage if provided\n",
    "    if args.kappa_second_stage_vae is not None:\n",
    "        with open(os.path.join(args.kappa_second_stage_vae, \"model_hparams.json\"), \"r\") as f:\n",
    "            kappa_vae2_hparams = json.load(f)\n",
    "        kappa_vae2 = VAESecondStage(**kappa_vae2_hparams)\n",
    "        ckpt1 = tf.train.Checkpoint(step=tf.Variable(1), net=kappa_vae2)\n",
    "        checkpoint_manager1 = tf.train.CheckpointManager(ckpt1, args.kappa_second_stage_vae, 1)\n",
    "        checkpoint_manager1.checkpoint.restore(checkpoint_manager1.latest_checkpoint).expect_partial()\n",
    "        kappa_vae2.trainable = False\n",
    "        kappa_vae2.encoder.trainable = False\n",
    "        kappa_vae2.decoder.trainable = False\n",
    "        kappa_sampling_function = lambda batch_size: 10 ** kappa_vae.decode(kappa_vae2.sample(batch_size))\n",
    "    else:\n",
    "        kappa_sampling_function = lambda batch_size: 10 ** kappa_vae.sample(batch_size)\n",
    "\n",
    "    # =============== source vae ========================================\n",
    "    # Load first stage and freeze weights\n",
    "    with open(os.path.join(args.source_first_stage_vae, \"model_hparams.json\"), \"r\") as f:\n",
    "        source_vae_hparams = json.load(f)\n",
    "    source_vae = VAE(**source_vae_hparams)\n",
    "    ckpt1 = tf.train.Checkpoint(step=tf.Variable(1), net=source_vae)\n",
    "    checkpoint_manager1 = tf.train.CheckpointManager(ckpt1, args.source_first_stage_vae, 1)\n",
    "    checkpoint_manager1.checkpoint.restore(checkpoint_manager1.latest_checkpoint).expect_partial()\n",
    "    source_vae.trainable = False\n",
    "    source_vae.encoder.trainable = False\n",
    "    source_vae.decoder.trainable = False\n",
    "\n",
    "    # Setup sampling from second stage if provided\n",
    "    if args.source_second_stage_vae is not None:\n",
    "        with open(os.path.join(args.source_second_stage_vae, \"model_hparams.json\"), \"r\") as f:\n",
    "            source_vae2_hparams = json.load(f)\n",
    "        source_vae2 = VAESecondStage(**source_vae2_hparams)\n",
    "        ckpt1 = tf.train.Checkpoint(step=tf.Variable(1), net=source_vae2)\n",
    "        checkpoint_manager1 = tf.train.CheckpointManager(ckpt1, args.source_second_stage_vae, 1)\n",
    "        checkpoint_manager1.checkpoint.restore(checkpoint_manager1.latest_checkpoint).expect_partial()\n",
    "        source_vae2.trainable = False\n",
    "        source_vae2.encoder.trainable = False\n",
    "        source_vae2.decoder.trainable = False\n",
    "        source_sampling_function = lambda batch_size: source_vae.decode(source_vae2.sample(batch_size))\n",
    "    else:\n",
    "        source_sampling_function = lambda batch_size: source_vae.sample(batch_size)\n",
    "\n",
    "    # with tf.device(rim_device):\n",
    "    phys = PhysicalModel(\n",
    "        pixels=args.image_pixels,\n",
    "        kappa_pixels=kappa_vae_hparams[\"pixels\"],\n",
    "        src_pixels=source_vae_hparams[\"pixels\"],\n",
    "        image_fov=args.image_fov,\n",
    "        kappa_fov=args.kappa_fov,\n",
    "        src_fov=args.source_fov,\n",
    "        method=args.forward_method,\n",
    "        noise_rms=args.noise_rms,\n",
    "        raytracer=raytracer,\n",
    "        psf_sigma=args.psf_sigma\n",
    "    )\n",
    "    kappa_model = UnetModel(\n",
    "        filters=args.kappa_filters,\n",
    "        filter_scaling=args.kappa_filter_scaling,\n",
    "        kernel_size=args.kappa_kernel_size,\n",
    "        layers=args.kappa_layers,\n",
    "        block_conv_layers=args.kappa_block_conv_layers,\n",
    "        strides=args.kappa_strides,\n",
    "        bottleneck_kernel_size=args.kappa_bottleneck_kernel_size,\n",
    "        bottleneck_filters=args.kappa_bottleneck_filters,\n",
    "        resampling_kernel_size=args.kappa_resampling_kernel_size,\n",
    "        gru_kernel_size=args.kappa_gru_kernel_size,\n",
    "        upsampling_interpolation=args.kappa_upsampling_interpolation,\n",
    "        kernel_regularizer_amp=args.kappa_kernel_regularizer_amp,\n",
    "        bias_regularizer_amp=args.kappa_bias_regularizer_amp,\n",
    "        activation=args.kappa_activation,\n",
    "        alpha=args.kappa_alpha,  # for leaky relu\n",
    "        initializer=args.kappa_initializer,\n",
    "    )\n",
    "    source_model = UnetModel(\n",
    "        filters=args.source_filters,\n",
    "        filter_scaling=args.source_filter_scaling,\n",
    "        kernel_size=args.source_kernel_size,\n",
    "        layers=args.source_layers,\n",
    "        block_conv_layers=args.source_block_conv_layers,\n",
    "        strides=args.source_strides,\n",
    "        bottleneck_kernel_size=args.source_bottleneck_kernel_size,\n",
    "        bottleneck_filters=args.source_bottleneck_filters,\n",
    "        resampling_kernel_size=args.source_resampling_kernel_size,\n",
    "        gru_kernel_size=args.source_gru_kernel_size,\n",
    "        upsampling_interpolation=args.source_upsampling_interpolation,\n",
    "        kernel_regularizer_amp=args.source_kernel_regularizer_amp,\n",
    "        bias_regularizer_amp=args.source_bias_regularizer_amp,\n",
    "        activation=args.source_activation,\n",
    "        alpha=args.source_alpha,  # for leaky relu\n",
    "        initializer=args.source_initializer,\n",
    "    )\n",
    "    rim = RIMUnet(\n",
    "        physical_model=phys,\n",
    "        source_model=source_model,\n",
    "        kappa_model=kappa_model,\n",
    "        steps=args.steps,\n",
    "        adam=args.adam,\n",
    "        kappalog=args.kappalog,\n",
    "        source_link=args.source_link,\n",
    "        kappa_normalize=args.kappa_normalize,\n",
    "        kappa_init=args.kappa_init,\n",
    "        source_init=args.source_init\n",
    "    )\n",
    "    learning_rate_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate=args.initial_learning_rate,\n",
    "        decay_rate=args.decay_rate,\n",
    "        decay_steps=args.decay_steps,\n",
    "        staircase=args.staircase\n",
    "    )\n",
    "    optim = tf.keras.optimizers.deserialize(\n",
    "        {\n",
    "            \"class_name\": args.optimizer,\n",
    "            'config': {\"learning_rate\": learning_rate_schedule}\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # ==== Take care of where to write logs and stuff =================================================================\n",
    "    if args.model_id.lower() != \"none\":\n",
    "        logname = args.model_id\n",
    "    elif args.logname is not None:\n",
    "        logname = args.logname\n",
    "    else:\n",
    "        logname = args.logname_prefixe + datetime.now().strftime(\"%y-%m-%d_%H-%M-%S\")\n",
    "    if args.logdir.lower() != \"none\":\n",
    "        logdir = os.path.join(args.logdir, logname)\n",
    "        if not os.path.isdir(logdir):\n",
    "            os.mkdir(logdir)\n",
    "        writer = tf.summary.create_file_writer(logdir)\n",
    "    else:\n",
    "        writer = nullwriter()\n",
    "    # ===== Make sure directory and checkpoint manager are created to save model ===================================\n",
    "    if args.model_dir.lower() != \"none\":\n",
    "        models_dir = os.path.join(args.model_dir, logname)\n",
    "        if not os.path.isdir(models_dir):\n",
    "            os.mkdir(models_dir)\n",
    "            with open(os.path.join(models_dir, \"script_params.json\"), \"w\") as f:\n",
    "                json.dump(vars(args), f, indent=4)\n",
    "        source_checkpoints_dir = os.path.join(models_dir, \"source_checkpoints\")\n",
    "        if not os.path.isdir(source_checkpoints_dir):\n",
    "            os.mkdir(source_checkpoints_dir)\n",
    "            with open(os.path.join(source_checkpoints_dir, \"hparams.json\"), \"w\") as f:\n",
    "                hparams_dict = {key: vars(args)[\"source_\" + key] for key in SOURCE_MODEL_HPARAMS}\n",
    "                json.dump(hparams_dict, f, indent=4)\n",
    "        kappa_checkpoints_dir = os.path.join(models_dir, \"kappa_checkpoints\")\n",
    "        if not os.path.isdir(kappa_checkpoints_dir):\n",
    "            os.mkdir(kappa_checkpoints_dir)\n",
    "            with open(os.path.join(kappa_checkpoints_dir, \"hparams.json\"), \"w\") as f:\n",
    "                hparams_dict = {key: vars(args)[\"kappa_\" + key] for key in KAPPA_MODEL_HPARAMS}\n",
    "                json.dump(hparams_dict, f, indent=4)\n",
    "        source_ckpt = tf.train.Checkpoint(step=tf.Variable(1), optimizer=optim, net=rim.source_model)\n",
    "        source_checkpoint_manager = tf.train.CheckpointManager(source_ckpt, source_checkpoints_dir, max_to_keep=args.max_to_keep)\n",
    "        kappa_ckpt = tf.train.Checkpoint(step=tf.Variable(1), optimizer=optim, net=rim.kappa_model)\n",
    "        kappa_checkpoint_manager = tf.train.CheckpointManager(kappa_ckpt, kappa_checkpoints_dir, max_to_keep=args.max_to_keep)\n",
    "        save_checkpoint = True\n",
    "        # ======= Load model if model_id is provided ===============================================================\n",
    "        if args.model_id.lower() != \"none\":\n",
    "            if args.load_checkpoint == \"lastest\":\n",
    "                kappa_checkpoint_manager.checkpoint.restore(kappa_checkpoint_manager.latest_checkpoint)\n",
    "                source_checkpoint_manager.checkpoint.restore(source_checkpoint_manager.latest_checkpoint)\n",
    "            elif args.load_checkpoint == \"best\":\n",
    "                kappa_scores = np.loadtxt(os.path.join(kappa_checkpoints_dir, \"score_sheet.txt\"))\n",
    "                source_scores = np.loadtxt(os.path.join(source_checkpoints_dir, \"score_sheet.txt\"))\n",
    "                _kappa_checkpoint = kappa_scores[np.argmin(kappa_scores[:, 1]), 0]\n",
    "                _source_checkpoint = source_scores[np.argmin(source_scores[:, 1]), 0]\n",
    "                kappa_checkpoint = kappa_checkpoint_manager.checkpoints[_kappa_checkpoint]\n",
    "                kappa_checkpoint_manager.checkpoint.restore(kappa_checkpoint)\n",
    "                source_checkpoint = kappa_checkpoint_manager.checkpoints[_source_checkpoint]\n",
    "                source_checkpoint_manager.checkpoint.restore(source_checkpoint)\n",
    "            else:\n",
    "                kappa_checkpoint = kappa_checkpoint_manager.checkpoints[int(args.load_checkpoint)]\n",
    "                source_checkpoint = source_checkpoint_manager.checkpoints[int(args.load_checkpoint)]\n",
    "                kappa_checkpoint_manager.checkpoint.restore(kappa_checkpoint)\n",
    "                source_checkpoint_manager.checkpoint.restore(source_checkpoint)\n",
    "    else:\n",
    "        save_checkpoint = False\n",
    "    # =================================================================================================================\n",
    "\n",
    "    def train_step(X, source, kappa):\n",
    "        with tf.GradientTape(persistent=True, watch_accessed_variables=True) as tape:\n",
    "            tape.watch(rim.source_model.trainable_variables)\n",
    "            tape.watch(rim.kappa_model.trainable_variables)\n",
    "            source_series, kappa_series, chi_squared = rim.call(X, outer_tape=tape)\n",
    "            source_cost = tf.reduce_mean(tf.square(source_series - rim.source_inverse_link(source)), axis=(0, 2, 3, 4))\n",
    "            kappa_cost = tf.reduce_mean(tf.square(kappa_series - rim.kappa_inverse_link(kappa)), axis=(0, 2, 3, 4))\n",
    "            cost = tf.reduce_sum(kappa_cost + source_cost) / args.batch_size\n",
    "        gradient1 = tape.gradient(cost, rim.source_model.trainable_variables)\n",
    "        gradient2 = tape.gradient(cost, rim.kappa_model.trainable_variables)\n",
    "        if args.clipping:\n",
    "            gradient1 = [tf.clip_by_value(grad, -10, 10) for grad in gradient1]\n",
    "            gradient2 = [tf.clip_by_value(grad, -10, 10) for grad in gradient2]\n",
    "        optim.apply_gradients(zip(gradient1, rim.source_model.trainable_variables))\n",
    "        optim.apply_gradients(zip(gradient2, rim.kappa_model.trainable_variables))\n",
    "        chi_squared = tf.reduce_sum(chi_squared) / args.batch_size\n",
    "        source_cost = tf.reduce_sum(source_cost) / args.batch_size\n",
    "        kappa_cost = tf.reduce_sum(kappa_cost) / args.batch_size\n",
    "        return cost, chi_squared, source_cost, kappa_cost\n",
    "\n",
    "    # ====== Training loop ============================================================================================\n",
    "    epoch_loss = tf.metrics.Mean()\n",
    "    time_per_step = tf.metrics.Mean()\n",
    "    epoch_chi_squared = tf.metrics.Mean()\n",
    "    epoch_source_cost = tf.metrics.Mean()\n",
    "    epoch_kappa_cost = tf.metrics.Mean()\n",
    "    history = {  # recorded at the end of an epoch only\n",
    "        \"cost\": [],\n",
    "        \"chi_squared\": [],\n",
    "        \"learning_rate\": [],\n",
    "        \"time_per_step\": [],\n",
    "        \"source_cost\": [],\n",
    "        \"kappa_cost\": [],\n",
    "        \"step\": []\n",
    "    }\n",
    "    best_loss = np.inf\n",
    "    patience = args.patience\n",
    "    step = 0\n",
    "    global_start = time.time()\n",
    "    estimated_time_for_epoch = 0\n",
    "    out_of_time = False\n",
    "    lastest_checkpoint = 1\n",
    "    for epoch in range(args.epochs):\n",
    "        if (time.time() - global_start) > args.max_time*3600 - estimated_time_for_epoch:\n",
    "            break\n",
    "        epoch_start = time.time()\n",
    "        epoch_loss.reset_states()\n",
    "        epoch_chi_squared.reset_states()\n",
    "        epoch_source_cost.reset_states()\n",
    "        epoch_kappa_cost.reset_states()\n",
    "        time_per_step.reset_states()\n",
    "        with writer.as_default():\n",
    "            for batch in range(args.total_items // args.batch_size):\n",
    "                start = time.time()\n",
    "                # with tf.device(vae_device):\n",
    "                kappa = kappa_sampling_function(args.batch_size)\n",
    "                source = source_sampling_function(args.batch_size)\n",
    "                source /= tf.reduce_max(source, axis=(1, 2, 3), keepdims=True)  # preprocess source\n",
    "                X = tf.nn.relu(phys.noisy_forward(source, kappa, noise_rms=args.noise_rms))\n",
    "                # with tf.device(rim_device):\n",
    "                cost, chi_squared, source_cost, kappa_cost = train_step(X, source, kappa)\n",
    "        # ========== Summary and logs ==================================================================================\n",
    "                _time = time.time() - start\n",
    "                tf.summary.scalar(\"Time per step\", _time, step=step)\n",
    "                tf.summary.scalar(\"MSE\", cost, step=step)\n",
    "                tf.summary.scalar(\"Chi Squared\", chi_squared, step=step)\n",
    "                tf.summary.scalar(\"Source Cost\", source_cost, step=step)\n",
    "                tf.summary.scalar(\"Kappa Cost\", kappa_cost, step=step)\n",
    "                time_per_step.update_state([_time])\n",
    "                epoch_loss.update_state([cost])\n",
    "                epoch_chi_squared.update_state([chi_squared])\n",
    "                epoch_source_cost.update_state([source_cost])\n",
    "                epoch_kappa_cost.update_state([kappa_cost])\n",
    "                step += 1\n",
    "            if args.n_residuals > 0:\n",
    "                # with tf.device(vae_device):\n",
    "                kappa_true = kappa_sampling_function(args.n_residuals)\n",
    "                source_true = source_sampling_function(args.n_residuals)\n",
    "                lens_true = tf.nn.relu(phys.noisy_forward(source_true, kappa_true, noise_rms=args.noise_rms))\n",
    "                # with tf.device(rim_device):\n",
    "                source_pred, kappa_pred, chi_squared = rim.predict(lens_true)\n",
    "                lens_pred = phys.forward(source_pred[-1], kappa_pred[-1])\n",
    "            for res_idx in range(args.n_residuals):\n",
    "                try:\n",
    "                    tf.summary.image(f\"Residuals {res_idx}\",\n",
    "                                     plot_to_image(\n",
    "                                         residual_plot(\n",
    "                                             lens_true[res_idx],\n",
    "                                             source_true[res_idx, ...],\n",
    "                                             kappa_true[res_idx, ...],\n",
    "                                             lens_pred[res_idx],\n",
    "                                             source_pred[-1][res_idx, ...],\n",
    "                                             kappa_pred[-1][res_idx, ...],\n",
    "                                             chi_squared[-1][res_idx]\n",
    "                                         )), step=step)\n",
    "                except ValueError:\n",
    "                    continue\n",
    "\n",
    "            train_cost = epoch_loss.result().numpy()\n",
    "            train_chi_sq = epoch_chi_squared.result().numpy()\n",
    "            train_s_cost = epoch_source_cost.result().numpy()\n",
    "            train_k_cost = epoch_kappa_cost.result().numpy()\n",
    "            tf.summary.scalar(\"Learning Rate\", optim.lr(step), step=step)\n",
    "        print(f\"step {step} | train loss {train_cost:.3e} | chi sq {train_chi_sq:.3e}\"\n",
    "              f\"| learning rate {optim.lr(step).numpy():.2e} | time per step {time_per_step.result().numpy():.2e} s\")\n",
    "        history[\"cost\"].append(train_cost)\n",
    "        history[\"learning_rate\"].append(optim.lr(step).numpy())\n",
    "        history[\"chi_squared\"].append(train_chi_sq)\n",
    "        history[\"time_per_step\"].append(time_per_step.result().numpy())\n",
    "        history[\"kappa_cost\"].append(train_k_cost)\n",
    "        history[\"source_cost\"].append(train_s_cost)\n",
    "        history[\"step\"].append(step)\n",
    "\n",
    "        cost = train_cost\n",
    "        if np.isnan(cost):\n",
    "            print(\"Training broke the Universe\")\n",
    "            break\n",
    "        if cost < (1 - args.tolerance) * best_loss:\n",
    "            best_loss = cost\n",
    "            patience = args.patience\n",
    "        else:\n",
    "            patience -= 1\n",
    "        if (time.time() - global_start) > args.max_time * 3600:\n",
    "            out_of_time = True\n",
    "        if save_checkpoint:\n",
    "            source_checkpoint_manager.checkpoint.step.assign_add(1) # a bit of a hack\n",
    "            kappa_checkpoint_manager.checkpoint.step.assign_add(1)\n",
    "            if epoch % args.checkpoints == 0 or patience == 0 or epoch == args.epochs - 1 or out_of_time:\n",
    "                with open(os.path.join(kappa_checkpoints_dir, \"score_sheet.txt\"), mode=\"a\") as f:\n",
    "                    np.savetxt(f, np.array([[lastest_checkpoint, cost]]))\n",
    "                with open(os.path.join(source_checkpoints_dir, \"score_sheet.txt\"), mode=\"a\") as f:\n",
    "                    np.savetxt(f, np.array([[lastest_checkpoint, cost]]))\n",
    "                lastest_checkpoint += 1\n",
    "                source_checkpoint_manager.save()\n",
    "                kappa_checkpoint_manager.save()\n",
    "                print(\"Saved checkpoint for step {}: {}\".format(int(source_checkpoint_manager.checkpoint.step),\n",
    "                                                                source_checkpoint_manager.latest_checkpoint))\n",
    "        if patience == 0:\n",
    "            print(\"Reached patience\")\n",
    "            break\n",
    "        if out_of_time:\n",
    "            break\n",
    "        if epoch > 0:  # First epoch is always very slow and not a good estimate of an epoch time.\n",
    "            estimated_time_for_epoch = time.time() - epoch_start\n",
    "    return history, best_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6cd0c78-0b16-40d3-a888-dbb7dafc06dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import ArgumentParser\n",
    "    parser = ArgumentParser()\n",
    "    parser.add_argument(\"--model_id\",                   default=\"None\",                 help=\"Start from this model id checkpoint. None means start from scratch\")\n",
    "    parser.add_argument(\"--load_checkpoint\",            default=\"best\",                 help=\"One of 'best', 'lastest' or the specific checkpoint index.\")\n",
    "    parser.add_argument(\"--kappa_first_stage_vae\",      required=True)\n",
    "    parser.add_argument(\"--kappa_second_stage_vae\",     default=None)\n",
    "    parser.add_argument(\"--source_first_stage_vae\",     required=True)\n",
    "    parser.add_argument(\"--source_second_stage_vae\",    default=None)\n",
    "\n",
    "    # RIM hyperparameters\n",
    "    parser.add_argument(\"--steps\",                  default=16,     type=int,       help=\"Number of time steps of RIM\")\n",
    "    parser.add_argument(\"--adam\",                   action=\"store_true\",            help=\"ADAM update for the log-likelihood gradient.\")\n",
    "    parser.add_argument(\"--kappalog\",               action=\"store_true\")\n",
    "    parser.add_argument(\"--kappa_normalize\",        action=\"store_true\")\n",
    "    parser.add_argument(\"--source_link\",            default=\"identity\",             help=\"One of 'exp', 'source', 'relu' or 'identity' (default).\")\n",
    "    parser.add_argument(\"--kappa_init\",             default=1e-1,   type=float,     help=\"Initial value of kappa for RIM\")\n",
    "    parser.add_argument(\"--source_init\",            default=1e-3,   type=float,     help=\"Initial value of source for RIM\")\n",
    "\n",
    "    \n",
    "    # Kappa model hyperparameters\n",
    "    parser.add_argument(\"--kappa_filters\",                  default=32,     type=int)\n",
    "    parser.add_argument(\"--kappa_filter_scaling\",           default=1,      type=float)\n",
    "    parser.add_argument(\"--kappa_kernel_size\",              default=3,      type=int)\n",
    "    parser.add_argument(\"--kappa_layers\",                   default=2,      type=int)\n",
    "    parser.add_argument(\"--kappa_block_conv_layers\",        default=2,      type=int)\n",
    "    parser.add_argument(\"--kappa_strides\",                  default=2,      type=int)\n",
    "    parser.add_argument(\"--kappa_bottleneck_kernel_size\",   default=None,   type=int)\n",
    "    parser.add_argument(\"--kappa_bottleneck_filters\",       default=None,   type=int)\n",
    "    parser.add_argument(\"--kappa_resampling_kernel_size\",   default=None,   type=int)\n",
    "    parser.add_argument(\"--kappa_gru_kernel_size\",          default=None,   type=int)\n",
    "    parser.add_argument(\"--kappa_upsampling_interpolation\", action=\"store_true\")\n",
    "    parser.add_argument(\"--kappa_kernel_regularizer_amp\",   default=1e-4,   type=float)\n",
    "    parser.add_argument(\"--kappa_bias_regularizer_amp\",     default=1e-4,   type=float)\n",
    "    parser.add_argument(\"--kappa_activation\",               default=\"leaky_relu\")\n",
    "    parser.add_argument(\"--kappa_alpha\",                    default=0.1,    type=float)\n",
    "    parser.add_argument(\"--kappa_initializer\",              default=\"glorot_normal\")\n",
    "    \n",
    "    # Source model hyperparameters\n",
    "    parser.add_argument(\"--source_filters\",                  default=32,     type=int)\n",
    "    parser.add_argument(\"--source_filter_scaling\",           default=1,      type=float)\n",
    "    parser.add_argument(\"--source_kernel_size\",              default=3,      type=int)\n",
    "    parser.add_argument(\"--source_layers\",                   default=2,      type=int)\n",
    "    parser.add_argument(\"--source_block_conv_layers\",        default=2,      type=int)\n",
    "    parser.add_argument(\"--source_strides\",                  default=2,      type=int)\n",
    "    parser.add_argument(\"--source_bottleneck_kernel_size\",   default=None,   type=int)\n",
    "    parser.add_argument(\"--source_bottleneck_filters\",       default=None,   type=int)\n",
    "    parser.add_argument(\"--source_resampling_kernel_size\",   default=None,   type=int)\n",
    "    parser.add_argument(\"--source_gru_kernel_size\",          default=None,   type=int)\n",
    "    parser.add_argument(\"--source_upsampling_interpolation\", action=\"store_true\")\n",
    "    parser.add_argument(\"--source_kernel_regularizer_amp\",   default=1e-4,   type=float)\n",
    "    parser.add_argument(\"--source_bias_regularizer_amp\",     default=1e-4,   type=float)\n",
    "    parser.add_argument(\"--source_activation\",               default=\"leaky_relu\")\n",
    "    parser.add_argument(\"--source_alpha\",                    default=0.1,    type=float)\n",
    "    parser.add_argument(\"--source_initializer\",              default=\"glorot_normal\")\n",
    "\n",
    "    # Physical model hyperparameter\n",
    "    parser.add_argument(\"--forward_method\",         default=\"conv2d\",               help=\"One of ['conv2d', 'fft', 'unet']. If the option 'unet' is chosen, the parameter \"\n",
    "                                                                                         \"'--raytracer' must be provided and point to model checkpoint directory.\")\n",
    "    parser.add_argument(\"--raytracer\",              default=None,                   help=\"Path to raytracer checkpoint dir if method 'unet' is used.\")\n",
    "\n",
    "    # Training set params\n",
    "    parser.add_argument(\"-b\", \"--batch_size\",       default=1,      type=int,       help=\"Number of images in a batch. \")\n",
    "    parser.add_argument(\"--total_items\",            required=True,  type=int,       help=\"Total images in an epoch.\")\n",
    "    parser.add_argument(\"--image_pixels\",           default=512,    type=int,       help=\"Number of pixels on a side of the lensed image\")\n",
    "    parser.add_argument(\"--image_fov\",              default=20,     type=float,     help=\"Field of view of lensed image in arcsec\")\n",
    "    parser.add_argument(\"--kappa_fov\",              default=18,     type=float,     help=\"Field of view of kappa map (in lens plane), in arcsec\")\n",
    "    parser.add_argument(\"--source_fov\",             default=3,      type=float,     help=\"Field of view of source map, in arcsec\")\n",
    "    parser.add_argument(\"--noise_rms\",              default=1e-2,   type=float,     help=\"RMS of white noise added to lensed image\")\n",
    "    parser.add_argument(\"--psf_sigma\",              default=0.08,   type=float,     help=\"Size, in arcseconds, of the gaussian blurring PSF\")\n",
    "\n",
    "    # Optimization params\n",
    "    parser.add_argument(\"-e\", \"--epochs\",           default=10,     type=int,       help=\"Number of epochs for training.\")\n",
    "    parser.add_argument(\"--optimizer\",              default=\"Adam\",                 help=\"Class name of the optimizer (e.g. 'Adam' or 'Adamax')\")\n",
    "    parser.add_argument(\"--initial_learning_rate\",  default=1e-3,   type=float,     help=\"Initial learning rate.\")\n",
    "    parser.add_argument(\"--decay_rate\",             default=1.,     type=float,     help=\"Exponential decay rate of learning rate (1=no decay).\")\n",
    "    parser.add_argument(\"--decay_steps\",            default=1000,   type=int,       help=\"Decay steps of exponential decay of the learning rate.\")\n",
    "    parser.add_argument(\"--staircase\",              action=\"store_true\",            help=\"Learning rate schedule only change after decay steps if enabled.\")\n",
    "    parser.add_argument(\"--clipping\",               action=\"store_true\",            help=\"Clip backprop gradients between -10 and 10.\")\n",
    "    parser.add_argument(\"--patience\",               default=np.inf, type=int,       help=\"Number of step at which training is stopped if no improvement is recorder.\")\n",
    "    parser.add_argument(\"--tolerance\",              default=0,      type=float,     help=\"Current score <= (1 - tolerance) * best score => reset patience, else reduce patience.\")\n",
    "    parser.add_argument(\"--max_time\",               default=np.inf, type=float,     help=\"Time allowed for the training, in hours.\")\n",
    "\n",
    "\n",
    "    # logs\n",
    "    parser.add_argument(\"--logdir\",                  default=\"None\",                help=\"Path of logs directory. Default if None, no logs recorded.\")\n",
    "    parser.add_argument(\"--logname\",                 default=None,                  help=\"Overwrite name of the log with this argument\")\n",
    "    parser.add_argument(\"--logname_prefixe\",         default=\"RIMUnet512\",          help=\"If name of the log is not provided, this prefix is prepended to the date\")\n",
    "    parser.add_argument(\"--model_dir\",               default=\"None\",                help=\"Path to the directory where to save models checkpoints.\")\n",
    "    parser.add_argument(\"--checkpoints\",             default=10,    type=int,       help=\"Save a checkpoint of the models each {%} iteration.\")\n",
    "    parser.add_argument(\"--max_to_keep\",             default=3,     type=int,       help=\"Max model checkpoint to keep.\")\n",
    "    parser.add_argument(\"--n_residuals\",             default=1,     type=int,       help=\"Number of residual plots to save. Add overhead at the end of an epoch only.\")\n",
    "\n",
    "    # Reproducibility params\n",
    "    parser.add_argument(\"--seed\",                   default=None,   type=int,       help=\"Random seed for numpy and tensorflow.\")\n",
    "    parser.add_argument(\"--json_override\",          default=None,   nargs=\"+\",      help=\"A json filepath that will override every command line parameters. \"\n",
    "                                                                                 \"Useful for reproducibility\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4daa85ac-a656-4735-b431-0c2054ef1f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.argv = ['']\n",
    "args = parser.parse_args(\n",
    "  f\"kappa_first_stage_vae={os.getenv('CENSAI_PATH')}/models/VAE1_kappa_HPARAMS2_010_CL2_F64_NLbipolar_relu_LS32_210812184741 \"\\\n",
    "  f\"kappa_second_stage_vae={os.getenv('CENSAI_PATH')}/models/VAE1_kappa_HPARAMS2_010_CL2_F64_NLbipolar_relu_LS32_210812184741_second_stage_210813104442 \"\\\n",
    "  f\"source_first_stage_vae={os.getenv('CENSAI_PATH')}/models/VAE1_cosmos_HPARAMS_029_L3_CL4_F16_NLleaky_relu_LS128_ssi0.001_210810161842 \"\\\n",
    "  f\"source_second_stage_vae={os.getenv('CENSAI_PATH')}/models/VAE1_cosmos_HPARAMS_029_L3_CL4_F16_NLleaky_relu_LS128_ssi0.001_210810161842_second_stage_210812235647 \"\\\n",
    "  f\"forward_method=fft \"\\\n",
    "  f\"epochs=50 \"\\\n",
    "  f\"max_time=47 \"\\\n",
    "  f\"batch_size 1 \"\\\n",
    "  f\"total_items 200 \"\\\n",
    "  f\"image_pixels=512 \"\\\n",
    "  f\"image_fov=20 \"\\\n",
    "  f\"kappa_fov=20 \"\\\n",
    "  f\"source_fov=3 \"\\\n",
    "  f\"noise_rms=5e-2 \"\\\n",
    "  f\"psf_sigma=0.1 \"\\\n",
    "  f\"initial_learning_rate=1e-4 \"\\\n",
    "  f\"decay_rate=0.9 \"\\\n",
    "  f\"decay_steps=10000 \"\\\n",
    "  f\"staircase \"\\\n",
    "  f\"clipping \"\\\n",
    "  f\"patience=40 \"\\\n",
    "  f\"tolerance=0.01 \"\\\n",
    "  f\"steps=4 \"\\\n",
    "  f\"adam \"\\\n",
    "  f\"kappalog \"\\\n",
    "  f\"kappa_filters=32 \"\\\n",
    "  f\"kappa_filter_scaling=1 \"\\\n",
    "  f\"kappa_kernel_size=3 \"\\\n",
    "  f\"kappa_layers=3 \"\\\n",
    "  f\"kappa_block_conv_layers=2 \"\\\n",
    "  f\"kappa_strides=2 \"\\\n",
    "  f\"kappa_upsampling_interpolation \"\\\n",
    "  f\"kappa_kernel_regularizer_amp=1e-4 \"\\\n",
    "  f\"kappa_bias_regularizer_amp=1e-4 \"\\\n",
    "  f\"kappa_activatio=leaky_relu \"\\\n",
    "  f\"kappa_alpha=0.1 \"\\\n",
    "  f\"kappa_initializer=glorot_normal \"\\\n",
    "  f\"source_filters=32 \"\\\n",
    "  f\"source_filter_scaling=1 \"\\\n",
    "  f\"source_kernel_size=3 \"\\\n",
    "  f\"source_layers=3 \"\\\n",
    "  f\"source_block_conv_layers=2 \"\\\n",
    "  f\"source_strides=2 \"\\\n",
    "  f\"source_upsampling_interpolation \"\\\n",
    "  f\"source_kernel_regularizer_amp=1e-4 \"\\\n",
    "  f\"source_bias_regularizer_amp=1e-4 \"\\\n",
    "  f\"source_activatio=leaky_relu \"\\\n",
    "  f\"source_alpha=0.1 \"\\\n",
    "  f\"source_initializer=glorot_normal \"\\\n",
    "  f\"logdir=$HOME/scratch/Censai/logsRIMDU_wVAE \"\\\n",
    "  f\"logname_prefixe=RIMDU_wVAE \"\\\n",
    "  f\"model_dir=$HOME/scratch/Censai/models \"\\\n",
    "  f\"checkpoints=5 \"\\\n",
    "  f\"max_to_keep=3 \"\\\n",
    "  f\"n_residuals=0\".split()\n",
    ")\n",
    "if args.seed is not None:\n",
    "    tf.random.set_seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "cache_files = glob.glob(f\"{os.getenv('SLURM_TMPDIR')}/cache*\")\n",
    "for cache in cache_files:\n",
    "    os.remove(cache)\n",
    "rim, phys, train_dataset, val_dataset = main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5c93add-dfc4-4f73-8822-b9a5f07cb460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# b = 0\n",
    "# for batch, distributed_inputs in enumerate(train_dataset):\n",
    "#     if batch == b:\n",
    "#         for res_idx in range(args.batch_size):\n",
    "#             for t in range(args.steps):\n",
    "#                 lens_true = distributed_inputs[0][res_idx, ...]\n",
    "#                 source_true = distributed_inputs[1][res_idx, ...]\n",
    "#                 kappa_true = distributed_inputs[2][res_idx, ...]\n",
    "#                 source_pred, kappa_pred, chi_squared = rim.predict(lens_true[None, ...])\n",
    "#                 lens_pred = phys.forward(source_pred[t], kappa_pred[t])[0, ...]\n",
    "#                 fig = residual_plot(lens_true, source_true, kappa_true, lens_pred, source_pred[t][0, ...], kappa_pred[t][0, ...], chi_squared[t][0])\n",
    "#                 fig.suptitle(fr\"{batch}: Time step {t}, $\\chi^2 = ${chi_squared[t][0]:.2e}\")\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ee916b-9842-41e1-92d5-76a0bdb9a8d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "censai",
   "language": "python",
   "name": "censai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
