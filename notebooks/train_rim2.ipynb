{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e5d679e-2bb4-410e-a84b-bef211b11c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from censai import PhysicalModel, RIMUnet\n",
    "from censai.models import UnetModel\n",
    "from censai.data.lenses_tng import decode_train, decode_physical_model_info\n",
    "from censai.utils import nullwriter, rim_residual_plot as residual_plot, plot_to_image\n",
    "import os, glob, time\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "from datetime import datetime\n",
    "import random\n",
    "\"\"\" # NOTE ON THE USE OF MULTIPLE GPUS #\n",
    "Double the number of gpus will not speed up the code. In fact, doubling the number of gpus and mirroring \n",
    "the ops accross replicas means the code is TWICE as slow.\n",
    "\n",
    "In fact, using multiple gpus means one should at least multiply the batch size by the number of gpus introduced, \n",
    "and optimize hyperparameters accordingly (learning rate should be scaled similarly).\n",
    "\"\"\"\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if len(gpus) == 1:\n",
    "    STRATEGY = tf.distribute.OneDeviceStrategy(device=\"/gpu:0\")\n",
    "elif len(gpus) > 1:\n",
    "    STRATEGY = tf.distribute.MirroredStrategy()\n",
    "from censai.utils import nullcontext\n",
    "PHYSICAL_MODEL_DEVICE = nullcontext()\n",
    "wndb = False\n",
    "\n",
    "\n",
    "\n",
    "RIM_HPARAMS = [\n",
    "    \"adam\",\n",
    "    \"steps\",\n",
    "    \"kappalog\",\n",
    "    \"kappa_normalize\"\n",
    "]\n",
    "SOURCE_MODEL_HPARAMS = [\n",
    "    \"filters\",\n",
    "    \"filter_scaling\",\n",
    "    \"kernel_size\",\n",
    "    \"layers\",\n",
    "    \"block_conv_layers\",\n",
    "    \"strides\",\n",
    "    \"bottleneck_kernel_size\",\n",
    "    \"bottleneck_filters\",\n",
    "    \"resampling_kernel_size\",\n",
    "    \"gru_kernel_size\",\n",
    "    \"upsampling_interpolation\",\n",
    "    \"kernel_regularizer_amp\",\n",
    "    \"bias_regularizer_amp\",\n",
    "    \"activation\",\n",
    "    \"alpha\",\n",
    "    \"initializer\"\n",
    "]\n",
    "KAPPA_MODEL_HPARAMS = [\n",
    "    \"filters\",\n",
    "    \"filter_scaling\",\n",
    "    \"kernel_size\",\n",
    "    \"layers\",\n",
    "    \"block_conv_layers\",\n",
    "    \"strides\",\n",
    "    \"bottleneck_kernel_size\",\n",
    "    \"bottleneck_filters\",\n",
    "    \"resampling_kernel_size\",\n",
    "    \"gru_kernel_size\",\n",
    "    \"upsampling_interpolation\",\n",
    "    \"kernel_regularizer_amp\",\n",
    "    \"bias_regularizer_amp\",\n",
    "    \"activation\",\n",
    "    \"alpha\",\n",
    "    \"initializer\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee734b7b-f53d-4823-ae30-9fecc36003f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def main(args):\n",
    "    if wndb:\n",
    "        config = wandb.config\n",
    "        config.update(vars(args))\n",
    "    files = []\n",
    "    for dataset in args.datasets:\n",
    "        files.extend(glob.glob(os.path.join(dataset, \"*.tfrecords\")))\n",
    "    random.shuffle(files)\n",
    "    # Read concurrently from multiple records\n",
    "    files = tf.data.Dataset.from_tensor_slices(files)\n",
    "    dataset = files.interleave(lambda x: tf.data.TFRecordDataset(x, num_parallel_reads=args.num_parallel_reads, compression_type=args.compression_type),\n",
    "                               cycle_length=args.cycle_length, block_length=args.block_length)\n",
    "    # Read off global parameters from first example in dataset\n",
    "    for params in dataset.map(decode_physical_model_info):\n",
    "        break\n",
    "    dataset = dataset.map(decode_train).batch(args.batch_size)\n",
    "    # Do not prefetch in this script. Memory is more precious than latency\n",
    "    if args.cache_file is not None:\n",
    "        dataset = dataset.cache(args.cache_file)#.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    # else:  # do not cache if no file is provided, dataset is huge and does not fit in GPU or RAM\n",
    "    #     dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    train_dataset = dataset.take(int(args.train_split * args.total_items) // args.batch_size) # dont forget to divide by batch size!\n",
    "    val_dataset = dataset.skip(int(args.train_split * args.total_items) // args.batch_size)\n",
    "    val_dataset = val_dataset.take(int((1 - args.train_split) * args.total_items) // args.batch_size)\n",
    "    train_dataset = STRATEGY.experimental_distribute_dataset(train_dataset)\n",
    "    val_dataset = STRATEGY.experimental_distribute_dataset(val_dataset)\n",
    "    if args.raytracer is not None:\n",
    "        import json\n",
    "        with open(os.path.join(args.raytracer, \"ray_tracer_hparams.json\"), \"r\") as f:\n",
    "            raytracer_hparams = json.load(f)\n",
    "    else:\n",
    "        raytracer_hparams = {}\n",
    "    with STRATEGY.scope():  # Replicate ops accross gpus\n",
    "        phys = PhysicalModel(\n",
    "            pixels=params[\"kappa pixels\"].numpy(),\n",
    "            src_pixels=params[\"src pixels\"].numpy(),\n",
    "            image_fov=params[\"image fov\"].numpy(),\n",
    "            kappa_fov=params[\"kappa fov\"].numpy(),\n",
    "            method=args.forward_method,\n",
    "            noise_rms=params[\"noise rms\"].numpy(),\n",
    "            kappalog=args.kappalog,\n",
    "            checkpoint_path=args.raytracer,\n",
    "            device=PHYSICAL_MODEL_DEVICE,\n",
    "            **raytracer_hparams\n",
    "        )\n",
    "        if args.raytracer is not None:\n",
    "            # load last checkpoint in the checkpoint directory\n",
    "            checkpoint = tf.train.Checkpoint(net=phys.RayTracer)\n",
    "            manager = tf.train.CheckpointManager(checkpoint, directory=args.raytracer, max_to_keep=3)\n",
    "            checkpoint.restore(manager.latest_checkpoint)\n",
    "        kappa_model = UnetModel(\n",
    "            filters=args.kappa_filters,\n",
    "            filter_scaling=args.kappa_filter_scaling,\n",
    "            kernel_size=args.kappa_kernel_size,\n",
    "            layers=args.kappa_layers,\n",
    "            block_conv_layers=args.kappa_block_conv_layers,\n",
    "            strides=args.kappa_strides,\n",
    "            bottleneck_kernel_size=args.kappa_bottleneck_kernel_size,\n",
    "            bottleneck_filters=args.kappa_bottleneck_filters,\n",
    "            resampling_kernel_size=args.kappa_resampling_kernel_size,\n",
    "            gru_kernel_size=args.kappa_gru_kernel_size,\n",
    "            upsampling_interpolation=args.kappa_upsampling_interpolation,\n",
    "            kernel_regularizer_amp=args.kappa_kernel_regularizer_amp,\n",
    "            bias_regularizer_amp=args.kappa_bias_regularizer_amp,\n",
    "            activation=args.kappa_activation,\n",
    "            alpha=args.kappa_alpha,  # for leaky relu\n",
    "            initializer=args.kappa_initializer,\n",
    "        )\n",
    "        source_model = UnetModel(\n",
    "            filters=args.source_filters,\n",
    "            filter_scaling=args.source_filter_scaling,\n",
    "            kernel_size=args.source_kernel_size,\n",
    "            layers=args.source_layers,\n",
    "            block_conv_layers=args.source_block_conv_layers,\n",
    "            strides=args.source_strides,\n",
    "            bottleneck_kernel_size=args.source_bottleneck_kernel_size,\n",
    "            bottleneck_filters=args.source_bottleneck_filters,\n",
    "            resampling_kernel_size=args.source_resampling_kernel_size,\n",
    "            gru_kernel_size=args.source_gru_kernel_size,\n",
    "            upsampling_interpolation=args.source_upsampling_interpolation,\n",
    "            kernel_regularizer_amp=args.source_kernel_regularizer_amp,\n",
    "            bias_regularizer_amp=args.source_bias_regularizer_amp,\n",
    "            activation=args.source_activation,\n",
    "            alpha=args.source_alpha,  # for leaky relu\n",
    "            initializer=args.source_initializer,\n",
    "        )\n",
    "        rim = RIMUnet(\n",
    "            physical_model=phys,\n",
    "            source_model=source_model,\n",
    "            kappa_model=kappa_model,\n",
    "            steps=args.steps,\n",
    "            adam=args.adam,\n",
    "            kappalog=args.kappalog,\n",
    "            kappa_normalize=args.kappa_normalize\n",
    "        )\n",
    "        learning_rate_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "            initial_learning_rate=args.initial_learning_rate,\n",
    "            decay_rate=args.decay_rate,\n",
    "            decay_steps=args.decay_steps,\n",
    "            staircase=args.staircase\n",
    "        )\n",
    "        optim = tf.optimizers.Adam(learning_rate=learning_rate_schedule)\n",
    "\n",
    "    # ==== Take care of where to write logs and stuff =================================================================\n",
    "    if args.model_id.lower() != \"none\":\n",
    "        logname = args.model_id\n",
    "    elif args.logname is not None:\n",
    "        logname = args.logname\n",
    "    else:\n",
    "        logname = args.logname_prefixe + datetime.now().strftime(\"%y-%m-%d_%H-%M-%S\")\n",
    "    if args.logdir.lower() != \"none\":\n",
    "        logdir = os.path.join(args.logdir, logname)\n",
    "        traindir = os.path.join(logdir, \"train\")\n",
    "        testdir = os.path.join(logdir, \"test\")\n",
    "        if not os.path.isdir(logdir):\n",
    "            os.mkdir(logdir)\n",
    "        if not os.path.isdir(traindir):\n",
    "            os.mkdir(traindir)\n",
    "        if not os.path.isdir(testdir):\n",
    "            os.mkdir(testdir)\n",
    "        train_writer = tf.summary.create_file_writer(traindir)\n",
    "        test_writer = tf.summary.create_file_writer(testdir)\n",
    "    else:\n",
    "        test_writer = nullwriter()\n",
    "        train_writer = nullwriter()\n",
    "    # ===== Make sure directory and checkpoint manager are created to save model ===================================\n",
    "    if args.model_dir.lower() != \"none\":\n",
    "        models_dir = os.path.join(args.model_dir, logname)\n",
    "        if not os.path.isdir(models_dir):\n",
    "            os.mkdir(models_dir)\n",
    "            import json\n",
    "            with open(os.path.join(models_dir, \"script_params.json\"), \"w\") as f:\n",
    "                json.dump(vars(args), f)\n",
    "        source_checkpoints_dir = os.path.join(models_dir, \"source_checkpoints\")\n",
    "        if not os.path.isdir(source_checkpoints_dir):\n",
    "            os.mkdir(source_checkpoints_dir)\n",
    "            import json\n",
    "            with open(os.path.join(source_checkpoints_dir, \"hparams.json\"), \"w\") as f:\n",
    "                hparams_dict = {key: vars(args)[\"source_\" + key] for key in SOURCE_MODEL_HPARAMS}\n",
    "                json.dump(hparams_dict, f)\n",
    "        kappa_checkpoints_dir = os.path.join(models_dir, \"kappa_checkpoints\")\n",
    "        if not os.path.isdir(kappa_checkpoints_dir):\n",
    "            os.mkdir(kappa_checkpoints_dir)\n",
    "            import json\n",
    "            with open(os.path.join(kappa_checkpoints_dir, \"hparams.json\"), \"w\") as f:\n",
    "                hparams_dict = {key: vars(args)[\"kappa_\" + key] for key in KAPPA_MODEL_HPARAMS}\n",
    "                json.dump(hparams_dict, f)\n",
    "        source_ckpt = tf.train.Checkpoint(step=tf.Variable(1), optimizer=optim, net=rim.source_model)\n",
    "        source_checkpoint_manager = tf.train.CheckpointManager(source_ckpt, source_checkpoints_dir, max_to_keep=args.max_to_keep)\n",
    "        kappa_ckpt = tf.train.Checkpoint(step=tf.Variable(1), optimizer=optim, net=rim.kappa_model)\n",
    "        kappa_checkpoint_manager = tf.train.CheckpointManager(kappa_ckpt, kappa_checkpoints_dir, max_to_keep=args.max_to_keep)\n",
    "        save_checkpoint = True\n",
    "        # ======= Load model if model_id is provided ===============================================================\n",
    "        if args.model_id.lower() != \"none\":\n",
    "            if args.load_checkpoint == \"lastest\":\n",
    "                kappa_checkpoint_manager.checkpoint.restore(kappa_checkpoint_manager.latest_checkpoint)\n",
    "                source_checkpoint_manager.checkpoint.restore(source_checkpoint_manager.latest_checkpoint)\n",
    "            elif args.load_checkpoint == \"best\":\n",
    "                kappa_scores = np.loadtxt(os.path.join(kappa_checkpoints_dir, \"score_sheet.txt\"))\n",
    "                source_scores = np.loadtxt(os.path.join(source_checkpoints_dir, \"score_sheet.txt\"))\n",
    "                _kappa_checkpoint = kappa_scores[np.argmin(kappa_scores[:, 1]), 0]\n",
    "                _source_checkpoint = source_scores[np.argmin(source_scores[:, 1]), 0]\n",
    "                kappa_checkpoint = kappa_checkpoint_manager.checkpoints[_kappa_checkpoint]\n",
    "                kappa_checkpoint_manager.checkpoint.restore(kappa_checkpoint)\n",
    "                source_checkpoint = kappa_checkpoint_manager.checkpoints[_source_checkpoint]\n",
    "                source_checkpoint_manager.checkpoint.restore(source_checkpoint)\n",
    "            else:\n",
    "                kappa_checkpoint = kappa_checkpoint_manager.checkpoints[int(args.load_checkpoint)]\n",
    "                source_checkpoint = source_checkpoint_manager.checkpoints[int(args.load_checkpoint)]\n",
    "                kappa_checkpoint_manager.checkpoint.restore(kappa_checkpoint)\n",
    "                source_checkpoint_manager.checkpoint.restore(source_checkpoint)\n",
    "    else:\n",
    "        save_checkpoint = False\n",
    "    # =================================================================================================================\n",
    "\n",
    "    def train_step(inputs):\n",
    "        X, source, kappa = inputs\n",
    "        with tf.GradientTape(persistent=True, watch_accessed_variables=True) as tape:\n",
    "            tape.watch(rim.source_model.trainable_variables)\n",
    "            tape.watch(rim.kappa_model.trainable_variables)\n",
    "            cost = rim.cost_function(X, source, kappa, reduction=False)\n",
    "            cost = tf.reduce_sum(cost) / args.batch_size  # Reduce by the global batch size, not the replica batch size\n",
    "        gradient1 = tape.gradient(cost, rim.source_model.trainable_variables)\n",
    "        gradient2 = tape.gradient(cost, rim.kappa_model.trainable_variables)\n",
    "        if args.clipping:\n",
    "            gradient1 = [tf.clip_by_value(grad, -10, 10) for grad in gradient1]\n",
    "            gradient2 = [tf.clip_by_value(grad, -10, 10) for grad in gradient2]\n",
    "        optim.apply_gradients(zip(gradient1, rim.source_model.trainable_variables))\n",
    "        optim.apply_gradients(zip(gradient2, rim.kappa_model.trainable_variables))\n",
    "        return cost\n",
    "\n",
    "    @tf.function\n",
    "    def distributed_train_step(dist_inputs):\n",
    "        per_replica_losses = STRATEGY.run(train_step, args=(dist_inputs,))\n",
    "        # Replica losses are aggregated by summing them\n",
    "        return STRATEGY.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses, axis=None)\n",
    "\n",
    "    def test_step(inputs):\n",
    "        X, source, kappa = inputs\n",
    "        cost = rim.cost_function(X, source, kappa, reduction=False)\n",
    "        cost = tf.reduce_sum(cost) / args.batch_size\n",
    "        return cost\n",
    "\n",
    "    @tf.function\n",
    "    def distributed_test_step(dist_inputs):\n",
    "        per_replica_losses = STRATEGY.run(test_step, args=(dist_inputs,))\n",
    "        # Replica losses are aggregated by summing them\n",
    "        return STRATEGY.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses, axis=None)\n",
    "\n",
    "    # ====== Training loop ============================================================================================\n",
    "    epoch_loss = tf.metrics.Mean()\n",
    "    time_per_step = tf.metrics.Mean()\n",
    "    val_loss = tf.metrics.Mean()\n",
    "    best_loss = np.inf\n",
    "    patience = args.patience\n",
    "    step = 0\n",
    "    lastest_checkpoint = 1\n",
    "    for epoch in range(args.epochs):\n",
    "        epoch_loss.reset_states()\n",
    "        time_per_step.reset_states()\n",
    "        with train_writer.as_default():\n",
    "            for batch, distributed_inputs in enumerate(train_dataset):\n",
    "                start = time.time()\n",
    "                cost = distributed_train_step(distributed_inputs)\n",
    "                # ========== Summary and logs ==========\n",
    "                _time = time.time() - start\n",
    "                tf.summary.scalar(\"Time per step\", _time, step=step)\n",
    "                time_per_step.update_state([_time])\n",
    "                epoch_loss.update_state([cost])\n",
    "                tf.summary.scalar(\"MSE\", cost, step=step)\n",
    "                step += 1\n",
    "            tf.summary.scalar(\"Learning Rate\", optim.lr(step), step=step)\n",
    "            # last batch we make a summary of residuals\n",
    "            for res_idx in range(min(args.n_residuals, args.batch_size)):\n",
    "                lens_true = distributed_inputs[0][res_idx, ...]\n",
    "                source_true = distributed_inputs[1][res_idx, ...]\n",
    "                kappa_true = distributed_inputs[2][res_idx, ...]\n",
    "                source_pred, kappa_pred, chi_squared = rim.call(lens_true[None, ...])\n",
    "                lens_pred = phys.forward(source_pred[-1], kappa_pred[-1])[0, ...]\n",
    "                tf.summary.image(f\"Residual {res_idx}\",\n",
    "                                 plot_to_image(\n",
    "                                     residual_plot(\n",
    "                                         lens_true, source_true, rim.kappa_link(kappa_true), lens_pred, source_pred[-1][0, ...],\n",
    "                                         kappa_pred[-1][0, ...], chi_squared\n",
    "                                     )), step=step)\n",
    "#         with test_writer.as_default():\n",
    "#             val_loss.reset_states()\n",
    "#             for distributed_inputs in val_dataset:\n",
    "#                 test_cost = distributed_test_step(distributed_inputs)\n",
    "#                 val_loss.update_state([test_cost])\n",
    "#             for res_idx in range(min(args.n_residuals, args.batch_size)):\n",
    "#                 lens_true = distributed_inputs[0][res_idx, ...]\n",
    "#                 source_true = distributed_inputs[1][res_idx, ...]\n",
    "#                 kappa_true = distributed_inputs[2][res_idx, ...]\n",
    "#                 source_pred, kappa_pred, chi_squared = rim.call(lens_true[None, ...])\n",
    "#                 lens_pred = phys.forward(source_pred[-1], kappa_pred[-1])[0, ...]\n",
    "#                 tf.summary.image(f\"Residual {res_idx}\",\n",
    "#                                  plot_to_image(\n",
    "#                                      residual_plot(\n",
    "#                                          lens_true, source_true, kappa_true, lens_pred, source_pred[-1][0, ...],\n",
    "#                                          kappa_pred[-1][0, ...], chi_squared\n",
    "#                                      )), step=step)\n",
    "#             tf.summary.scalar(\"MSE\", test_cost, step=step)\n",
    "#         val_cost = val_loss.result().numpy()\n",
    "        train_cost = epoch_loss.result().numpy()\n",
    "        val_cost = train_cost\n",
    "        print(f\"epoch {epoch} | train loss {train_cost:.3e} | val loss {val_cost:.3e} \"\n",
    "              f\"| learning rate {optim.lr(step).numpy():.2e} | time per step {time_per_step.result().numpy():.2e} s\")\n",
    "        if val_cost < (1 - args.tolerance) * best_loss:\n",
    "            best_loss = val_cost\n",
    "            patience = args.patience\n",
    "        else:\n",
    "            patience -= 1\n",
    "        if save_checkpoint:\n",
    "            source_checkpoint_manager.checkpoint.step.assign_add(1) # a bit of a hack\n",
    "            kappa_checkpoint_manager.checkpoint.step.assign_add(1)\n",
    "            if epoch % args.checkpoints == 0 or patience == 0 or epoch == args.epochs - 1:\n",
    "                with open(os.path.join(kappa_checkpoints_dir, \"score_sheet.txt\"), mode=\"a\") as f:\n",
    "                    np.savetxt(f, np.array([[lastest_checkpoint, val_cost]]))\n",
    "                with open(os.path.join(source_checkpoints_dir, \"score_sheet.txt\"), mode=\"a\") as f:\n",
    "                    np.savetxt(f, np.array([[lastest_checkpoint, val_cost]]))\n",
    "                lastest_checkpoint += 1\n",
    "                source_checkpoint_manager.save()\n",
    "                kappa_checkpoint_manager.save()\n",
    "                print(\"Saved checkpoint for step {}: {}\".format(int(source_checkpoint_manager.checkpoint.step),\n",
    "                                                                source_checkpoint_manager.latest_checkpoint))\n",
    "        if patience == 0:\n",
    "            print(\"Reached patience\")\n",
    "            break\n",
    "#         with tf.summary.create_file_writer(os.path.join(args.logdir, args.logname_prefixe + \"_source_hparams\")).as_default():\n",
    "#             hparams_dict = {key: vars(args)[\"source_\" +key] for key in SOURCE_MODEL_HPARAMS}\n",
    "#             hp.hparams(hparams_dict)\n",
    "#             tf.summary.scalar(\"Test MSE\", best_loss, step=step)\n",
    "#             tf.summary.scalar(\"Final Train MSE\", train_cost, step=step)\n",
    "\n",
    "#         with tf.summary.create_file_writer(os.path.join(args.logdir, args.logname_prefixe + \"_kappa_hparams\")).as_default():\n",
    "#             hparams_dict = {key: vars(args)[\"kappa_\" + key] for key in KAPPA_MODEL_HPARAMS}\n",
    "#             hp.hparams(hparams_dict)\n",
    "#             tf.summary.scalar(\"Test MSE\", best_loss, step=step)\n",
    "#             tf.summary.scalar(\"Final Train MSE\", train_cost, step=step)\n",
    "\n",
    "#         with tf.summary.create_file_writer(os.path.join(args.logdir, args.logname_prefixe + \"_rim_hparams\")).as_default():\n",
    "#             hparams_dict = {key: vars(args)[key] for key in RIM_HPARAMS}\n",
    "#             hp.hparams(hparams_dict)\n",
    "#             tf.summary.scalar(\"Test MSE\", best_loss, step=step)\n",
    "#             tf.summary.scalar(\"Final Train MSE\", train_cost, step=step)\n",
    "    return rim, phys, train_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e65afbb5-bc2a-4f8e-b0b2-100aff716878",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--json_override'], dest='json_override', nargs=None, const=None, default=None, type=None, choices=None, help='A json filepath that will override every command line parameters. Useful for reproducibility', metavar=None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from argparse import ArgumentParser\n",
    "import json\n",
    "parser = ArgumentParser()\n",
    "parser.add_argument(\"--model_id\",               default=\"None\",                 help=\"Start from this model id checkpoint. None means start from scratch\")\n",
    "parser.add_argument(\"--load_checkpoint\",        default=\"best\",                 help=\"One of 'best', 'lastest' or the specific checkpoint index.\")\n",
    "parser.add_argument(\"--datasets\",               required=True,  nargs=\"+\",      help=\"Path to directories that contains tfrecords of dataset. Can be multiple inputs (space separated)\")\n",
    "parser.add_argument(\"--compression_type\",       default=None,                   help=\"Compression type used to write data. Default assumes no compression.\")\n",
    "\n",
    "# RIM hyperparameters\n",
    "parser.add_argument(\"--steps\",                  default=16,     type=int,       help=\"Number of time steps of RIM\")\n",
    "parser.add_argument(\"--adam\",                   action=\"store_true\",            help=\"ADAM update for the log-likelihood gradient.\")\n",
    "parser.add_argument(\"--kappalog\",               action=\"store_true\")\n",
    "parser.add_argument(\"--kappa_normalize\",        action=\"store_true\")\n",
    "\n",
    "# Kappa model hyperparameters\n",
    "parser.add_argument(\"--kappa_filters\",                  default=32,     type=int)\n",
    "parser.add_argument(\"--kappa_filter_scaling\",           default=1,      type=int)\n",
    "parser.add_argument(\"--kappa_kernel_size\",              default=3,      type=int)\n",
    "parser.add_argument(\"--kappa_layers\",                   default=2,      type=int)\n",
    "parser.add_argument(\"--kappa_block_conv_layers\",        default=2,      type=int)\n",
    "parser.add_argument(\"--kappa_strides\",                  default=2,      type=int)\n",
    "parser.add_argument(\"--kappa_bottleneck_kernel_size\",   default=None,   type=int)\n",
    "parser.add_argument(\"--kappa_bottleneck_filters\",       default=None,   type=int)\n",
    "parser.add_argument(\"--kappa_resampling_kernel_size\",   default=None,   type=int)\n",
    "parser.add_argument(\"--kappa_gru_kernel_size\",          default=None,   type=int)\n",
    "parser.add_argument(\"--kappa_upsampling_interpolation\", action=\"store_true\")\n",
    "parser.add_argument(\"--kappa_kernel_regularizer_amp\",   default=1e-4,   type=float)\n",
    "parser.add_argument(\"--kappa_bias_regularizer_amp\",     default=1e-4,   type=float)\n",
    "parser.add_argument(\"--kappa_activation\",               default=\"leaky_relu\")\n",
    "parser.add_argument(\"--kappa_alpha\",                    default=0.1,    type=float)\n",
    "parser.add_argument(\"--kappa_initializer\",              default=\"glorot_normal\")\n",
    "\n",
    "# Source model hyperparameters\n",
    "parser.add_argument(\"--source_filters\",                  default=32,     type=int)\n",
    "parser.add_argument(\"--source_filter_scaling\",           default=1,      type=int)\n",
    "parser.add_argument(\"--source_kernel_size\",              default=3,      type=int)\n",
    "parser.add_argument(\"--source_layers\",                   default=2,      type=int)\n",
    "parser.add_argument(\"--source_block_conv_layers\",        default=2,      type=int)\n",
    "parser.add_argument(\"--source_strides\",                  default=2,      type=int)\n",
    "parser.add_argument(\"--source_bottleneck_kernel_size\",   default=None,   type=int)\n",
    "parser.add_argument(\"--source_bottleneck_filters\",       default=None,   type=int)\n",
    "parser.add_argument(\"--source_resampling_kernel_size\",   default=None,   type=int)\n",
    "parser.add_argument(\"--source_gru_kernel_size\",          default=None,   type=int)\n",
    "parser.add_argument(\"--source_upsampling_interpolation\", action=\"store_true\")\n",
    "parser.add_argument(\"--source_kernel_regularizer_amp\",   default=1e-4,   type=float)\n",
    "parser.add_argument(\"--source_bias_regularizer_amp\",     default=1e-4,   type=float)\n",
    "parser.add_argument(\"--source_activation\",               default=\"leaky_relu\")\n",
    "parser.add_argument(\"--source_alpha\",                    default=0.1,    type=float)\n",
    "parser.add_argument(\"--source_initializer\",              default=\"glorot_normal\")\n",
    "\n",
    "# Physical model hyperparameter\n",
    "parser.add_argument(\"--forward_method\",         default=\"conv2d\",               help=\"One of ['conv2d', 'fft', 'unet']. If the option 'unet' is chosen, the parameter \"\n",
    "                                                                                     \"'--raytracer' must be provided and point to model checkpoint directory.\")\n",
    "parser.add_argument(\"--raytracer\",              default=None,                   help=\"Path to raytracer checkpoint dir if method 'unet' is used.\")\n",
    "\n",
    "# Training set params\n",
    "parser.add_argument(\"-b\", \"--batch_size\",       default=1,      type=int,       help=\"Number of images in a batch. \")\n",
    "parser.add_argument(\"--train_split\",            default=0.8,    type=float,     help=\"Fraction of the training set.\")\n",
    "parser.add_argument(\"--total_items\",            required=True,  type=int,       help=\"Total images in an epoch.\")\n",
    "# ... for tfrecord dataset\n",
    "parser.add_argument(\"--num_parallel_reads\",     default=10,     type=int,       help=\"TFRecord dataset number of parallel reads when loading data.\")\n",
    "parser.add_argument(\"--cache_file\",             default=None,                   help=\"Path to cache file, useful when training on server. Use ${SLURM_TMPDIR}/cache\")\n",
    "parser.add_argument(\"--cycle_length\",           default=4,      type=int,       help=\"Number of files to read concurrently.\")\n",
    "parser.add_argument(\"--block_length\",           default=1,      type=int,       help=\"Number of example to read from each files.\")\n",
    "\n",
    "# Optimization params\n",
    "parser.add_argument(\"-e\", \"--epochs\",           default=10,     type=int,       help=\"Number of epochs for training.\")\n",
    "parser.add_argument(\"--initial_learning_rate\",  default=1e-3,   type=float,     help=\"Initial learning rate.\")\n",
    "parser.add_argument(\"--decay_rate\",             default=1.,     type=float,     help=\"Exponential decay rate of learning rate (1=no decay).\")\n",
    "parser.add_argument(\"--decay_steps\",            default=1000,   type=int,       help=\"Decay steps of exponential decay of the learning rate.\")\n",
    "parser.add_argument(\"--staircase\",              action=\"store_true\",            help=\"Learning rate schedule only change after decay steps if enabled.\")\n",
    "parser.add_argument(\"--clipping\",               action=\"store_true\",            help=\"Clip backprop gradients between -10 and 10.\")\n",
    "parser.add_argument(\"--patience\",               default=np.inf, type=int,       help=\"Number of step at which training is stopped if no improvement is recorder.\")\n",
    "parser.add_argument(\"--tolerance\",              default=0,      type=float,     help=\"Current score <= (1 - tolerance) * best score => reset patience, else reduce patience.\")\n",
    "\n",
    "# logs\n",
    "parser.add_argument(\"--logdir\",                  default=\"None\",                help=\"Path of logs directory. Default if None, no logs recorded.\")\n",
    "parser.add_argument(\"--logname\",                 default=None,                  help=\"Overwrite name of the log with this argument\")\n",
    "parser.add_argument(\"--logname_prefixe\",         default=\"RIMUnet512\",          help=\"If name of the log is not provided, this prefix is prepended to the date\")\n",
    "parser.add_argument(\"--model_dir\",               default=\"None\",                help=\"Path to the directory where to save models checkpoints.\")\n",
    "parser.add_argument(\"--checkpoints\",             default=10,    type=int,       help=\"Save a checkpoint of the models each {%} iteration.\")\n",
    "parser.add_argument(\"--max_to_keep\",             default=3,     type=int,       help=\"Max model checkpoint to keep.\")\n",
    "parser.add_argument(\"--n_residuals\",             default=1,     type=int,       help=\"Number of residual plots to save. Add overhead at the end of an epoch only.\")\n",
    "\n",
    "# Reproducibility params\n",
    "parser.add_argument(\"--seed\",                   default=None,   type=int,       help=\"Random seed for numpy and tensorflow.\")\n",
    "parser.add_argument(\"--json_override\",          default=None,                   help=\"A json filepath that will override every command line parameters. \"\n",
    "                                                                                     \"Useful for reproducibility\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4daa85ac-a656-4735-b431-0c2054ef1f8f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ResourceExhaustedError",
     "evalue": "in user code:\n\n    <ipython-input-2-e7aa0275e2e4>:194 distributed_train_step  *\n        per_replica_losses = STRATEGY.run(train_step, args=(dist_inputs,))\n    <ipython-input-2-e7aa0275e2e4>:181 train_step  *\n        cost = rim.cost_function(X, source, kappa, reduction=False)\n    /lustre04/scratch/aadam/Censai/censai/rim_unet.py:133 cost_function  *\n        source_series, kappa_series, _ = self.call(lensed_image)\n    /lustre04/scratch/aadam/Censai/censai/rim_unet.py:105 call  *\n        output_1, state_1, output_2, state_2 = self.time_step(source_init, state_1, grads[0], kappa_init, state_2, grads[1])\n    /lustre04/scratch/aadam/Censai/censai/rim_unet.py:86 time_step  *\n        xt_2, ht_2 = self.kappa_model(inputs_2, state_2, grad_2)\n    /lustre04/scratch/aadam/Censai/censai/models/rim_unet_model.py:116 __call__  *\n        return self.call(xt, states, grad)\n    /lustre04/scratch/aadam/Censai/censai/models/rim_unet_model.py:133 call  *\n        delta_xt, new_state = self.bottleneck_gru(delta_xt, states[-1])\n    /lustre04/scratch/aadam/Censai/censai/models/layers/conv_gru_component.py:32 call  *\n        gru_2_out  = self.gru2(gru_1_outE, ht_12)\n    /lustre04/scratch/aadam/Censai/censai/models/layers/conv_gru.py:38 call  *\n        z = self.update_gate(stacked_input)  # Update gate vector\n    /home/aadam/environments/censai3.8/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py:1008 __call__  **\n        self._maybe_build(inputs)\n    /home/aadam/environments/censai3.8/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py:2710 _maybe_build\n        self.build(input_shapes)  # pylint:disable=not-callable\n    /home/aadam/environments/censai3.8/lib/python3.8/site-packages/tensorflow/python/keras/layers/convolutional.py:198 build\n        self.kernel = self.add_weight(\n    /home/aadam/environments/censai3.8/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py:623 add_weight\n        variable = self._add_variable_with_custom_getter(\n    /home/aadam/environments/censai3.8/lib/python3.8/site-packages/tensorflow/python/training/tracking/base.py:805 _add_variable_with_custom_getter\n        new_variable = getter(\n    /home/aadam/environments/censai3.8/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer_utils.py:130 make_variable\n        return tf_variables.VariableV1(\n    /home/aadam/environments/censai3.8/lib/python3.8/site-packages/tensorflow/python/ops/variables.py:260 __call__\n        return cls._variable_v1_call(*args, **kwargs)\n    /home/aadam/environments/censai3.8/lib/python3.8/site-packages/tensorflow/python/ops/variables.py:206 _variable_v1_call\n        return previous_getter(\n    /home/aadam/environments/censai3.8/lib/python3.8/site-packages/tensorflow/python/ops/variables.py:67 getter\n        return captured_getter(captured_previous, **kwargs)\n    /home/aadam/environments/censai3.8/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:2083 creator_with_resource_vars\n        created = self._create_variable(next_creator, **kwargs)\n    /home/aadam/environments/censai3.8/lib/python3.8/site-packages/tensorflow/python/distribute/one_device_strategy.py:278 _create_variable\n        return next_creator(**kwargs)\n    /home/aadam/environments/censai3.8/lib/python3.8/site-packages/tensorflow/python/ops/variables.py:67 getter\n        return captured_getter(captured_previous, **kwargs)\n    /home/aadam/environments/censai3.8/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py:712 variable_capturing_scope\n        v = UnliftedInitializerVariable(\n    /home/aadam/environments/censai3.8/lib/python3.8/site-packages/tensorflow/python/ops/variables.py:264 __call__\n        return super(VariableMetaclass, cls).__call__(*args, **kwargs)\n    /home/aadam/environments/censai3.8/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py:227 __init__\n        initial_value = initial_value()\n    /home/aadam/environments/censai3.8/lib/python3.8/site-packages/tensorflow/python/keras/initializers/initializers_v2.py:409 __call__\n        return super(VarianceScaling, self).__call__(\n    /home/aadam/environments/censai3.8/lib/python3.8/site-packages/tensorflow/python/ops/init_ops_v2.py:600 __call__\n        return self._random_generator.random_uniform(shape, -limit, limit, dtype)\n    /home/aadam/environments/censai3.8/lib/python3.8/site-packages/tensorflow/python/ops/init_ops_v2.py:1081 random_uniform\n        return op(\n    /home/aadam/environments/censai3.8/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    /home/aadam/environments/censai3.8/lib/python3.8/site-packages/tensorflow/python/ops/random_ops.py:308 random_uniform\n        result = math_ops.add(result * (maxval - minval), minval, name=name)\n    /home/aadam/environments/censai3.8/lib/python3.8/site-packages/tensorflow/python/ops/math_ops.py:1164 binary_op_wrapper\n        return func(x, y, name=name)\n    /home/aadam/environments/censai3.8/lib/python3.8/site-packages/tensorflow/python/ops/math_ops.py:1496 _mul_dispatch\n        return multiply(x, y, name=name)\n    /home/aadam/environments/censai3.8/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    /home/aadam/environments/censai3.8/lib/python3.8/site-packages/tensorflow/python/ops/math_ops.py:518 multiply\n        return gen_math_ops.mul(x, y, name)\n    /home/aadam/environments/censai3.8/lib/python3.8/site-packages/tensorflow/python/ops/gen_math_ops.py:6068 mul\n        _ops.raise_from_not_ok_status(e, name)\n    /home/aadam/environments/censai3.8/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:6862 raise_from_not_ok_status\n        six.raise_from(core._status_to_exception(e.code, message), None)\n    <string>:3 raise_from\n        \n\n    ResourceExhaustedError: OOM when allocating tensor with shape[5,5,6144,2048] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:Mul]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-b9dbe6952c93>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcache\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcache_files\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m \u001b[0mrim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-e7aa0275e2e4>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    222\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistributed_inputs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m                 \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdistributed_train_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistributed_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m                 \u001b[0;31m# ========== Summary and logs ==========\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m                 \u001b[0m_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/environments/censai3.8/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/environments/censai3.8/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    869\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 871\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    872\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/environments/censai3.8/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    723\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_deleter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFunctionDeleter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lifted_initializer_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m     self._concrete_stateful_fn = (\n\u001b[0;32m--> 725\u001b[0;31m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    726\u001b[0m             *args, **kwds))\n\u001b[1;32m    727\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/environments/censai3.8/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2967\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2968\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2969\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2970\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2971\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/environments/censai3.8/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3360\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3361\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3362\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/environments/censai3.8/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3194\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3195\u001b[0m     graph_function = ConcreteFunction(\n\u001b[0;32m-> 3196\u001b[0;31m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[1;32m   3197\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3198\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/environments/censai3.8/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 990\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    991\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/environments/censai3.8/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    635\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/environments/censai3.8/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    975\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    976\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 977\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    978\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    979\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: in user code:\n\n    <ipython-input-2-e7aa0275e2e4>:194 distributed_train_step  *\n        per_replica_losses = STRATEGY.run(train_step, args=(dist_inputs,))\n    <ipython-input-2-e7aa0275e2e4>:181 train_step  *\n        cost = rim.cost_function(X, source, kappa, reduction=False)\n    /lustre04/scratch/aadam/Censai/censai/rim_unet.py:133 cost_function  *\n        source_series, kappa_series, _ = self.call(lensed_image)\n    /lustre04/scratch/aadam/Censai/censai/rim_unet.py:105 call  *\n        output_1, state_1, output_2, state_2 = self.time_step(source_init, state_1, grads[0], kappa_init, state_2, grads[1])\n    /lustre04/scratch/aadam/Censai/censai/rim_unet.py:86 time_step  *\n        xt_2, ht_2 = self.kappa_model(inputs_2, state_2, grad_2)\n    /lustre04/scratch/aadam/Censai/censai/models/rim_unet_model.py:116 __call__  *\n        return self.call(xt, states, grad)\n    /lustre04/scratch/aadam/Censai/censai/models/rim_unet_model.py:133 call  *\n        delta_xt, new_state = self.bottleneck_gru(delta_xt, states[-1])\n    /lustre04/scratch/aadam/Censai/censai/models/layers/conv_gru_component.py:32 call  *\n        gru_2_out  = self.gru2(gru_1_outE, ht_12)\n    /lustre04/scratch/aadam/Censai/censai/models/layers/conv_gru.py:38 call  *\n        z = self.update_gate(stacked_input)  # Update gate vector\n    /home/aadam/environments/censai3.8/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py:1008 __call__  **\n        self._maybe_build(inputs)\n    /home/aadam/environments/censai3.8/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py:2710 _maybe_build\n        self.build(input_shapes)  # pylint:disable=not-callable\n    /home/aadam/environments/censai3.8/lib/python3.8/site-packages/tensorflow/python/keras/layers/convolutional.py:198 build\n        self.kernel = self.add_weight(\n    /home/aadam/environments/censai3.8/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py:623 add_weight\n        variable = self._add_variable_with_custom_getter(\n    /home/aadam/environments/censai3.8/lib/python3.8/site-packages/tensorflow/python/training/tracking/base.py:805 _add_variable_with_custom_getter\n        new_variable = getter(\n    /home/aadam/environments/censai3.8/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer_utils.py:130 make_variable\n        return tf_variables.VariableV1(\n    /home/aadam/environments/censai3.8/lib/python3.8/site-packages/tensorflow/python/ops/variables.py:260 __call__\n        return cls._variable_v1_call(*args, **kwargs)\n    /home/aadam/environments/censai3.8/lib/python3.8/site-packages/tensorflow/python/ops/variables.py:206 _variable_v1_call\n        return previous_getter(\n    /home/aadam/environments/censai3.8/lib/python3.8/site-packages/tensorflow/python/ops/variables.py:67 getter\n        return captured_getter(captured_previous, **kwargs)\n    /home/aadam/environments/censai3.8/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:2083 creator_with_resource_vars\n        created = self._create_variable(next_creator, **kwargs)\n    /home/aadam/environments/censai3.8/lib/python3.8/site-packages/tensorflow/python/distribute/one_device_strategy.py:278 _create_variable\n        return next_creator(**kwargs)\n    /home/aadam/environments/censai3.8/lib/python3.8/site-packages/tensorflow/python/ops/variables.py:67 getter\n        return captured_getter(captured_previous, **kwargs)\n    /home/aadam/environments/censai3.8/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py:712 variable_capturing_scope\n        v = UnliftedInitializerVariable(\n    /home/aadam/environments/censai3.8/lib/python3.8/site-packages/tensorflow/python/ops/variables.py:264 __call__\n        return super(VariableMetaclass, cls).__call__(*args, **kwargs)\n    /home/aadam/environments/censai3.8/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py:227 __init__\n        initial_value = initial_value()\n    /home/aadam/environments/censai3.8/lib/python3.8/site-packages/tensorflow/python/keras/initializers/initializers_v2.py:409 __call__\n        return super(VarianceScaling, self).__call__(\n    /home/aadam/environments/censai3.8/lib/python3.8/site-packages/tensorflow/python/ops/init_ops_v2.py:600 __call__\n        return self._random_generator.random_uniform(shape, -limit, limit, dtype)\n    /home/aadam/environments/censai3.8/lib/python3.8/site-packages/tensorflow/python/ops/init_ops_v2.py:1081 random_uniform\n        return op(\n    /home/aadam/environments/censai3.8/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    /home/aadam/environments/censai3.8/lib/python3.8/site-packages/tensorflow/python/ops/random_ops.py:308 random_uniform\n        result = math_ops.add(result * (maxval - minval), minval, name=name)\n    /home/aadam/environments/censai3.8/lib/python3.8/site-packages/tensorflow/python/ops/math_ops.py:1164 binary_op_wrapper\n        return func(x, y, name=name)\n    /home/aadam/environments/censai3.8/lib/python3.8/site-packages/tensorflow/python/ops/math_ops.py:1496 _mul_dispatch\n        return multiply(x, y, name=name)\n    /home/aadam/environments/censai3.8/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    /home/aadam/environments/censai3.8/lib/python3.8/site-packages/tensorflow/python/ops/math_ops.py:518 multiply\n        return gen_math_ops.mul(x, y, name)\n    /home/aadam/environments/censai3.8/lib/python3.8/site-packages/tensorflow/python/ops/gen_math_ops.py:6068 mul\n        _ops.raise_from_not_ok_status(e, name)\n    /home/aadam/environments/censai3.8/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:6862 raise_from_not_ok_status\n        six.raise_from(core._status_to_exception(e.code, message), None)\n    <string>:3 raise_from\n        \n\n    ResourceExhaustedError: OOM when allocating tensor with shape[5,5,6144,2048] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:Mul]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.argv = ['']\n",
    "args = parser.parse_args(\n",
    "#   f\"--datasets {os.getenv('HOME')}/scratch/Censai/data/lenses128_TNG100 {os.getenv('HOME')}/scratch/Censai/data/lenses128_NIS \"\\\n",
    "  f\"--datasets {os.getenv('HOME')}/scratch/Censai/data/lenses128_TNG100 \"\\\n",
    "  f\"--compression_type=GZIP \"\\\n",
    "  f\"--forward_method=conv2d \"\\\n",
    "  f\"--epochs=1000 \"\\\n",
    "  f\"--initial_learning_rate=1e-4 \"\\\n",
    "  f\"--decay_rate=1 \"\\\n",
    "  f\"--decay_steps=10 \"\\\n",
    "  f\"--staircase \"\\\n",
    "  f\"--clipping \"\\\n",
    "  f\"--patience=40 \"\\\n",
    "  f\"--tolerance=0.01 \"\\\n",
    "  f\"--batch_size=1 \"\\\n",
    "  f\"--train_split=1 \"\\\n",
    "  f\"--total_items=1 \"\\\n",
    "  f\"--num_parallel_reads=1 \"\\\n",
    "  f\"--cycle_length=1 \"\\\n",
    "  f\"--block_length=1 \"\\\n",
    "  f\"--steps=26 \"\\\n",
    "  f\"--adam \"\\\n",
    "  f\"--kappalog \"\\\n",
    "  f\"--kappa_filters=32 \"\\\n",
    "  f\"--kappa_filter_scaling=2 \"\\\n",
    "  f\"--kappa_kernel_size=3 \"\\\n",
    "  f\"--kappa_layers=5 \"\\\n",
    "  f\"--kappa_block_conv_layers=3 \"\\\n",
    "  f\"--kappa_strides=2 \"\\\n",
    "  f\"--kappa_upsampling_interpolation \"\\\n",
    "  f\"--kappa_kernel_regularizer_amp=1e-4 \"\\\n",
    "  f\"--kappa_bias_regularizer_amp=1e-4 \"\\\n",
    "  f\"--kappa_activatio=leaky_relu \"\\\n",
    "  f\"--kappa_alpha=0.1 \"\\\n",
    "  f\"--kappa_initializer=glorot_normal \"\\\n",
    "  f\"--source_filters=32 \"\\\n",
    "  f\"--source_filter_scaling=2 \"\\\n",
    "  f\"--source_kernel_size=3 \"\\\n",
    "  f\"--source_layers=5 \"\\\n",
    "  f\"--source_block_conv_layers=3 \"\\\n",
    "  f\"--source_strides=2 \"\\\n",
    "  f\"--source_upsampling_interpolation \"\\\n",
    "  f\"--source_kernel_regularizer_amp=1e-4 \"\\\n",
    "  f\"--source_bias_regularizer_amp=1e-4 \"\\\n",
    "  f\"--source_activatio=leaky_relu \"\\\n",
    "  f\"--source_alpha=0.1 \"\\\n",
    "  f\"--source_initializer=glorot_normal \"\\\n",
    "  f\"--cache_file={os.getenv('SLURM_TMPDIR')}/cache \"\\\n",
    "  f\"--logdir={os.getenv('HOME')}/scratch/Censai/logs \"\\\n",
    "  f\"--logname_prefixe=RIM_Unet128_Interactive \"\\\n",
    "#   f\"--model_dir={os.getenv('HOME')}/scratch/Censai/models \"\\\n",
    "#   f\"--checkpoints=5 \"\\\n",
    "#   f\"--max_to_keep=10 \"\\\n",
    "  f\"--n_residuals=0\".split()\n",
    ")\n",
    "cache_files = glob.glob(f\"{os.getenv('SLURM_TMPDIR')}/cache*\")\n",
    "for cache in cache_files:\n",
    "    os.remove(cache)\n",
    "rim, phys, train_dataset, val_dataset = main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c93add-dfc4-4f73-8822-b9a5f07cb460",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch, distributed_inputs in enumerate(train_dataset):\n",
    "    for res_idx in range(args.batch_size):\n",
    "        for t in range(args.steps):\n",
    "            lens_true = distributed_inputs[0][res_idx, ...]\n",
    "            source_true = distributed_inputs[1][res_idx, ...]\n",
    "            kappa_true = distributed_inputs[2][res_idx, ...]\n",
    "            source_pred, kappa_pred, chi_squared = rim.call(lens_true[None, ...])\n",
    "            lens_pred = phys.forward(source_pred[t], kappa_pred[t])[0, ...]\n",
    "            fig = residual_plot(lens_true, source_true, rim.kappa_link(kappa_true), lens_pred, source_pred[t][0, ...], kappa_pred[t][0, ...], chi_squared)\n",
    "            fig.suptitle(fr\"Time step {t}, $\\chi^2 = ${chi_squared:.2e}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d88eb9d-d48c-4509-813f-a524abcdff28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "censai",
   "language": "python",
   "name": "censai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
