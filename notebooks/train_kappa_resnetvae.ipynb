{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a9ee99d-317c-42a5-93b1-be9ceb7dbabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from censai.data.kappa_tng import decode_train, decode_shape\n",
    "from censai.utils import nullwriter, vae_residual_plot as residual_plot, plot_to_image\n",
    "from censai.definitions import PolynomialSchedule, log_10, DTYPE\n",
    "from censai import ResnetVAE\n",
    "import os, glob, json\n",
    "import math\n",
    "from datetime import datetime\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "VAE_HPARAMS = [\n",
    "    \"pixels\",\n",
    "    \"layers\",\n",
    "    \"res_blocks_in_layer\",\n",
    "    \"conv_layers_per_block\",\n",
    "    \"filter_scaling\",\n",
    "    \"filters\",\n",
    "    \"kernel_size\",\n",
    "    \"res_architecture\",\n",
    "    \"kernel_reg_amp\",\n",
    "    \"bias_reg_amp\",\n",
    "    \"activation\",\n",
    "    \"dropout_rate\",\n",
    "    \"batch_norm\",\n",
    "    \"latent_size\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2b65bd5-2f20-4bcd-b989-3c28876f8d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    files = []\n",
    "    for dataset in args.datasets:\n",
    "        files.extend(glob.glob(os.path.join(dataset, \"*.tfrecords\")))\n",
    "    np.random.shuffle(files)\n",
    "    # Read concurrently from multiple records\n",
    "    files = tf.data.Dataset.from_tensor_slices(files)\n",
    "    dataset = files.interleave(lambda x: tf.data.TFRecordDataset(x, compression_type=args.compression_type),\n",
    "                               block_length=args.block_length, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    # Read off global parameters from first example in dataset\n",
    "    for pixels in dataset.map(decode_shape):\n",
    "        break\n",
    "    vars(args).update({\"pixels\": int(pixels)})\n",
    "    dataset = dataset.map(decode_train).map(log_10).batch(args.batch_size)\n",
    "    if args.cache_file is not None:\n",
    "        dataset = dataset.cache(args.cache_file).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    else:\n",
    "        dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    train_dataset = dataset.take(math.floor(args.train_split * args.total_items / args.batch_size)) # dont forget to divide by batch size!\n",
    "    val_dataset = dataset.skip(math.floor(args.train_split * args.total_items / args.batch_size))\n",
    "    val_dataset = val_dataset.take(math.ceil((1 - args.train_split) * args.total_items / args.batch_size))\n",
    "\n",
    "    if isinstance(args.res_blocks_in_layer, list):\n",
    "        if len(args.res_blocks_in_layer) == 1:\n",
    "            vars(args).update({\"res_blocks_in_layer\": args.res_blocks_in_layer[0]})\n",
    "    vae = ResnetVAE(\n",
    "        pixels=pixels,\n",
    "        layers=args.layers,\n",
    "        res_blocks_in_layer=args.res_blocks_in_layer,\n",
    "        conv_layers_per_block=args.conv_layers_per_block,\n",
    "        filter_scaling=args.filter_scaling,\n",
    "        filters=args.filters,\n",
    "        kernel_size=args.kernel_size,\n",
    "        res_architecture=args.res_architecture,\n",
    "        kernel_reg_amp=args.kernel_reg_amp,\n",
    "        bias_reg_amp=args.bias_reg_amp,\n",
    "        activation=args.activation,\n",
    "        dropout_rate=args.dropout_rate,\n",
    "        batch_norm=args.batch_norm,\n",
    "        latent_size=args.latent_size\n",
    "    )\n",
    "    vae.call(tf.zeros(shape=[args.batch_size, pixels, pixels, 1], dtype=DTYPE))  # build layers of model\n",
    "    learning_rate_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate=args.initial_learning_rate,\n",
    "        decay_rate=args.decay_rate,\n",
    "        decay_steps=args.decay_steps,\n",
    "        staircase=args.staircase\n",
    "    )\n",
    "    beta_schedule = PolynomialSchedule(initial_value=args.beta_init, end_value=args.beta_end_value, power=args.beta_decay_power, decay_steps=args.beta_decay_steps, cyclical=args.beta_cyclical)\n",
    "    skip_strength_schedule = PolynomialSchedule(initial_value=args.skip_strength_init, end_value=0., power=args.skip_strength_decay_power, decay_steps=args.skip_strength_decay_steps)\n",
    "    l2_bottleneck_schedule = PolynomialSchedule(initial_value=args.l2_bottleneck_init, end_value=0., power=args.l2_bottleneck_decay_power, decay_steps=args.l2_bottleneck_decay_steps)\n",
    "    optim = tf.keras.optimizers.deserialize(\n",
    "        {\n",
    "            \"class_name\": args.optimizer,\n",
    "            'config': {\"learning_rate\": learning_rate_schedule}\n",
    "        }\n",
    "    )\n",
    "    # ==== Take care of where to write logs and stuff =================================================================\n",
    "    if args.model_id.lower() != \"none\":\n",
    "        logname = args.model_id\n",
    "    elif args.logname is not None:\n",
    "        logname = args.logname\n",
    "    else:\n",
    "        logname = args.logname_prefixe + \"_\" + datetime.now().strftime(\"%y%m%d%H%M%S\")\n",
    "    if args.logdir.lower() != \"none\":\n",
    "        logdir = os.path.join(args.logdir, logname)\n",
    "        if not os.path.isdir(logdir):\n",
    "            os.mkdir(logdir)\n",
    "        writer = tf.summary.create_file_writer(logdir)\n",
    "    else:\n",
    "        writer = nullwriter()\n",
    "    # ===== Make sure directory and checkpoint manager are created to save model ===================================\n",
    "    if args.model_dir.lower() != \"none\":\n",
    "        checkpoints_dir = os.path.join(args.model_dir, logname)\n",
    "        if not os.path.isdir(checkpoints_dir):\n",
    "            os.mkdir(checkpoints_dir)\n",
    "            with open(os.path.join(checkpoints_dir, \"script_params.json\"), \"w\") as f:\n",
    "                json.dump(vars(args), f, indent=4)\n",
    "            with open(os.path.join(checkpoints_dir, \"model_hparams.json\"), \"w\") as f:\n",
    "                hparams_dict = {key: vars(args)[key] for key in VAE_HPARAMS}\n",
    "                json.dump(hparams_dict, f, indent=4)\n",
    "        ckpt = tf.train.Checkpoint(step=tf.Variable(1), optimizer=optim, net=vae)\n",
    "        checkpoint_manager = tf.train.CheckpointManager(ckpt, checkpoints_dir, max_to_keep=args.max_to_keep)\n",
    "        save_checkpoint = True\n",
    "        # ======= Load model if model_id is provided ===============================================================\n",
    "        if args.model_id.lower() != \"none\":\n",
    "            if args.load_checkpoint == \"lastest\":\n",
    "                checkpoint_manager.checkpoint.restore(checkpoint_manager.latest_checkpoint)\n",
    "            elif args.load_checkpoint == \"best\":\n",
    "                scores = np.loadtxt(os.path.join(checkpoints_dir, \"score_sheet.txt\"))\n",
    "                _checkpoint = scores[np.argmin(scores[:, 1]), 0]\n",
    "                checkpoint = checkpoint_manager.checkpoints[_checkpoint]\n",
    "                checkpoint_manager.checkpoint.restore(checkpoint)\n",
    "            else:\n",
    "                checkpoint = checkpoint_manager.checkpoints[int(args.load_checkpoint)]\n",
    "                checkpoint_manager.checkpoint.restore(checkpoint)\n",
    "    else:\n",
    "        save_checkpoint = False\n",
    "\n",
    "    def train_step(x, step):\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(vae.trainable_weights)\n",
    "            reconstruction_loss, kl_loss, bottleneck_l2_loss = vae.cost_function_training(x,  skip_strength_schedule(step), l2_bottleneck_schedule(step))\n",
    "            cost = tf.reduce_sum(reconstruction_loss + beta_schedule(step) * kl_loss + bottleneck_l2_loss) / args.batch_size\n",
    "        gradients = tape.gradient(cost, vae.trainable_weights)\n",
    "        if args.clipping:\n",
    "            gradients = [tf.clip_by_value(grad, -10, 10) for grad in gradients]\n",
    "        optim.apply_gradients(zip(gradients, vae.trainable_weights))\n",
    "        return cost, reconstruction_loss, kl_loss\n",
    "\n",
    "    def test_step(x,  step):\n",
    "        reconstruction_loss, kl_loss, bottleneck_l2_loss = vae.cost_function_training(x, skip_strength_schedule(step), l2_bottleneck_schedule(step))\n",
    "        cost = tf.reduce_sum(reconstruction_loss + beta_schedule(step) * kl_loss + bottleneck_l2_loss) / args.batch_size\n",
    "        return cost, reconstruction_loss, kl_loss\n",
    "\n",
    "    # ====== Training loop ============================================================================================\n",
    "    epoch_loss = tf.metrics.Mean()\n",
    "    epoch_reconstruction_loss = tf.metrics.Mean()\n",
    "    epoch_kl_loss = tf.metrics.Mean()\n",
    "    time_per_step = tf.metrics.Mean()\n",
    "    val_loss = tf.metrics.Mean()\n",
    "    val_reconstruction_loss = tf.metrics.Mean()\n",
    "    val_kl_loss = tf.metrics.Mean()\n",
    "    history = {  # recorded at the end of an epoch only\n",
    "        \"train_cost\": [],\n",
    "        \"val_cost\": [],\n",
    "        \"learning_rate\": [],\n",
    "        \"time_per_step\": [],\n",
    "        \"train_reconstruction_loss\": [],\n",
    "        \"val_reconstruction_loss\": [],\n",
    "        \"train_kl_loss\": [],\n",
    "        \"val_kl_loss\": []\n",
    "    }\n",
    "    best_loss = np.inf\n",
    "    patience = args.patience\n",
    "    step = 0\n",
    "    global_start = time.time()\n",
    "    estimated_time_for_epoch = 0\n",
    "    out_of_time = False\n",
    "    lastest_checkpoint = 1\n",
    "    for epoch in range(args.epochs):\n",
    "        if (time.time() - global_start) > args.max_time * 3600 - estimated_time_for_epoch:\n",
    "            break\n",
    "        epoch_start = time.time()\n",
    "        epoch_loss.reset_states()\n",
    "        epoch_reconstruction_loss.reset_states()\n",
    "        epoch_kl_loss.reset_states()\n",
    "        time_per_step.reset_states()\n",
    "        with writer.as_default():\n",
    "            for batch, x in enumerate(train_dataset):\n",
    "                start = time.time()\n",
    "                cost, reconstruction_loss, kl_loss = train_step(x, step=step)\n",
    "                # ========== Summary and logs ==================================================================================\n",
    "                _time = time.time() - start\n",
    "                tf.summary.scalar(\"Time per step\", _time, step=step)\n",
    "                tf.summary.scalar(\"MSE\", cost, step=step)\n",
    "                tf.summary.scalar(\"beta\", beta_schedule(step), step=step)\n",
    "                tf.summary.scalar(\"l2 bottleneck\", l2_bottleneck_schedule(step), step=step)\n",
    "                time_per_step.update_state([_time])\n",
    "                epoch_loss.update_state([cost])\n",
    "                epoch_reconstruction_loss.update_state([reconstruction_loss])\n",
    "                epoch_kl_loss.update_state([kl_loss])\n",
    "                step += 1\n",
    "            # last batch we make a summary of residuals\n",
    "            for res_idx in range(min(args.n_residuals, args.batch_size)):\n",
    "                kappa_true = x[res_idx, ...]\n",
    "                kappa_pred = vae.call(kappa_true[None, ...])[0, ...]\n",
    "                tf.summary.image(f\"Residuals {res_idx}\", plot_to_image(residual_plot(kappa_true, kappa_pred)), step=step)\n",
    "            # ========== Validation set ===================\n",
    "            val_loss.reset_states()\n",
    "            val_reconstruction_loss.reset_states()\n",
    "            val_kl_loss.reset_states()\n",
    "            for x in val_dataset:\n",
    "                cost, reconstruction_loss, kl_loss = test_step(x, step=step)\n",
    "                val_loss.update_state([cost])\n",
    "                val_reconstruction_loss.update_state([reconstruction_loss])\n",
    "                val_kl_loss.update_state([kl_loss])\n",
    "            for res_idx in range(min(args.n_residuals, args.batch_size)):\n",
    "                kappa_true = x[res_idx, ...]\n",
    "                kappa_pred = vae.call(kappa_true[None, ...])[0, ...]\n",
    "                tf.summary.image(f\"Val Residuals {res_idx}\", plot_to_image(residual_plot(kappa_true, kappa_pred)), step=step)\n",
    "\n",
    "            val_cost = val_loss.result().numpy()\n",
    "            train_cost = epoch_loss.result().numpy()\n",
    "            train_reconstruction_cost = epoch_reconstruction_loss.result().numpy()\n",
    "            val_reconstruction_cost = val_reconstruction_loss.result().numpy()\n",
    "            train_kl_cost = epoch_kl_loss.result().numpy()\n",
    "            val_kl_cost = val_kl_loss.result().numpy()\n",
    "            tf.summary.scalar(\"Val MSE\", val_cost, step=step)\n",
    "            tf.summary.scalar(\"Learning Rate\", optim.lr(step), step=step)\n",
    "        print(f\"epoch {epoch} | train loss {train_cost:.3e} | val loss {val_cost:.3e} \"\n",
    "              f\"| learning rate {optim.lr(step).numpy():.2e} | time per step {time_per_step.result().numpy():.2e} s \"\n",
    "              f\"| beta {beta_schedule(step):.2f} | skip strength {skip_strength_schedule(step):.2e} | l2_bottleneck {l2_bottleneck_schedule(step):.2e}\"\n",
    "             )\n",
    "        history[\"train_cost\"].append(train_cost)\n",
    "        history[\"val_cost\"].append(val_cost)\n",
    "        history[\"train_reconstruction_loss\"].append(train_reconstruction_cost)\n",
    "        history[\"val_reconstruction_loss\"].append(val_reconstruction_cost)\n",
    "        history[\"train_kl_loss\"].append(train_kl_cost)\n",
    "        history[\"val_kl_loss\"].append(val_kl_cost)\n",
    "        history[\"learning_rate\"].append(optim.lr(step).numpy())\n",
    "        history[\"time_per_step\"].append(time_per_step.result().numpy())\n",
    "\n",
    "        cost = train_cost if args.track_train else val_cost\n",
    "        if np.isnan(cost):\n",
    "            print(\"Training broke the Universe\")\n",
    "            break\n",
    "        if cost < (1 - args.tolerance) * best_loss:\n",
    "            best_loss = cost\n",
    "            patience = args.patience\n",
    "        else:\n",
    "            patience -= 1\n",
    "        if (time.time() - global_start) > args.max_time * 3600:\n",
    "            out_of_time = True\n",
    "        if save_checkpoint:\n",
    "            checkpoint_manager.checkpoint.step.assign_add(1) # a bit of a hack\n",
    "            if epoch % args.checkpoints == 0 or patience == 0 or epoch == args.epochs - 1 or out_of_time:\n",
    "                with open(os.path.join(checkpoints_dir, \"score_sheet.txt\"), mode=\"a\") as f:\n",
    "                    np.savetxt(f, np.array([[lastest_checkpoint, cost]]))\n",
    "                lastest_checkpoint += 1\n",
    "                checkpoint_manager.save()\n",
    "                print(\"Saved checkpoint for step {}: {}\".format(int(checkpoint_manager.checkpoint.step), checkpoint_manager.latest_checkpoint))\n",
    "        if patience == 0:\n",
    "            print(\"Reached patience\")\n",
    "            break\n",
    "        if out_of_time:\n",
    "            break\n",
    "        if epoch > 0:  # First epoch is always very slow and not a good estimate of an epoch time.\n",
    "            estimated_time_for_epoch = time.time() - epoch_start\n",
    "    print(f\"Finished training after {(time.time() - global_start)/3600:.3f} hours.\")\n",
    "    return history, vae, train_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ea910f8-1e53-41d1-9a02-d8ce8544d428",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--json_override'], dest='json_override', nargs='+', const=None, default=None, type=None, choices=None, help='A json filepath that will override every command line parameters. Useful for reproducibility', metavar=None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from argparse import ArgumentParser\n",
    "parser = ArgumentParser()\n",
    "parser.add_argument(\"--model_id\",               default=\"None\",                 help=\"Start from this model id checkpoint. None means start from scratch\")\n",
    "parser.add_argument(\"--load_checkpoint\",        default=\"best\",                 help=\"One of 'best', 'lastest' or the specific checkpoint index.\")\n",
    "parser.add_argument(\"--datasets\",               required=True, nargs=\"+\",       help=\"Path to kappa directories, with tfrecords files\")\n",
    "parser.add_argument(\"--compression_type\",       default=None,                   help=\"Compression type used to write data. Default assumes no compression.\")\n",
    "\n",
    "# Model params\n",
    "parser.add_argument(\"--layers\",                 default=4,              type=int,       help=\"Number of layer in encoder/decoder\")\n",
    "parser.add_argument(\"--res_blocks_in_layer\",    default=2, nargs=\"+\",   type=int,       help=\"List of res block per layers. If single number is given, assume uniform structure\")\n",
    "parser.add_argument(\"--conv_layers_per_block\",  default=2,              type=int,       help=\"Number of convolution layers in a block\")\n",
    "parser.add_argument(\"--filter_scaling\",         default=2,              type=float,     help=\"Filter scaling after each layers\")\n",
    "parser.add_argument(\"--filters\",                default=8,              type=int,       help=\"Number of filters in the first layer\")\n",
    "parser.add_argument(\"--kernel_size\",            default=3,              type=int)\n",
    "parser.add_argument(\"--res_architecture\",       default=\"bare\",                         help=\"One of ['bare', 'original', 'bn_after_addition', 'relu_before_addition', 'relu_only_pre_activation', 'full_pre_activation', 'full_pre_activation_rescale']\")\n",
    "parser.add_argument(\"--kernel_reg_amp\",         default=1e-4,           type=float,     help=\"L2 kernel regularization amplitude\")\n",
    "parser.add_argument(\"--bias_reg_amp\",           default=1e-4,           type=float,     help=\"L2 bias regularizer amplitude\")\n",
    "parser.add_argument(\"--activation\",             default=\"relu\",                         help=\"Name of activation function, on of ['relu', 'leaky_relu', 'bipolar_relu', 'bipolar_leaky_relu', 'bipolar_elu', 'gelu', etc.]\")\n",
    "parser.add_argument(\"--dropout_rate\",           default=None,           type=float,     help=\"2D spatial dropout rate (drop entire feature map to help them become independent)\")\n",
    "parser.add_argument(\"--batch_norm\",             default=0,              type=int,       help=\"0: False, do no use batch norm. 1: True, use batch norm beforce activation\")\n",
    "parser.add_argument(\"--latent_size\",            default=16,             type=int,       help=\"Twice the size of the latent code vector z\")\n",
    "\n",
    "# Training set params\n",
    "parser.add_argument(\"-b\", \"--batch_size\",       default=1,      type=int,       help=\"Number of images in a batch. \")\n",
    "parser.add_argument(\"--train_split\",            default=0.8,    type=float,     help=\"Fraction of the training set.\")\n",
    "parser.add_argument(\"--total_items\",            required=True,  type=int,       help=\"Total images in an epoch.\")\n",
    "# ... for tfrecord dataset\n",
    "parser.add_argument(\"--cache_file\",             default=None,                   help=\"Path to cache file, useful when training on server. Use ${SLURM_TMPDIR}/cache\")\n",
    "parser.add_argument(\"--block_length\",           default=1,      type=int,       help=\"Number of example to read from each files.\")\n",
    "\n",
    "# Optimization params\n",
    "parser.add_argument(\"-e\", \"--epochs\",                   default=10,     type=int,       help=\"Number of epochs for training.\")\n",
    "parser.add_argument(\"--optimizer\",                      default=\"Adam\",                 help=\"Class name of the optimizer (e.g. 'Adam' or 'Adamax')\")\n",
    "parser.add_argument(\"--initial_learning_rate\",          default=1e-3,   type=float,     help=\"Initial learning rate.\")\n",
    "parser.add_argument(\"--decay_rate\",                     default=1.,     type=float,     help=\"Exponential decay rate of learning rate (1=no decay).\")\n",
    "parser.add_argument(\"--decay_steps\",                    default=1000,   type=int,       help=\"Decay steps of exponential decay of the learning rate.\")\n",
    "parser.add_argument(\"--beta_init\",                      default=0.,     type=float,     help=\"Initial value of the beta schedule\")\n",
    "parser.add_argument(\"--beta_end_value\",                 default=1.,     type=float,     help=\"End value of the beta schedule\")\n",
    "parser.add_argument(\"--beta_decay_power\",               default=1.,     type=float,     help=\"Power of the Polynomial schedule\")\n",
    "parser.add_argument(\"--beta_decay_steps\",               default=1000,   type=int,       help=\"Number of steps until end of schedule is reached\")\n",
    "parser.add_argument(\"--beta_cyclical\",                  default=0,      type=int,       help=\"Make beta schedule cyclical if 1. 0: Monotone schedule.\")\n",
    "parser.add_argument(\"--skip_strength_init\",             default=1.,     type=float,     help=\"Initial value of the skip_strength schedule\")\n",
    "parser.add_argument(\"--skip_strength_end_value\",        default=0.,     type=float,     help=\"End value of the skip_strength schedule\")\n",
    "parser.add_argument(\"--skip_strength_decay_power\",      default=0.5,    type=float,     help=\"Power of the Polynomial schedule\")\n",
    "parser.add_argument(\"--skip_strength_decay_steps\",      default=1000,   type=int,       help=\"Number of steps until end of schedule is reached\")\n",
    "parser.add_argument(\"--l2_bottleneck_init\",             default=1.,     type=float,     help=\"Initial value of the l2_bottleneck schedule\")\n",
    "parser.add_argument(\"--l2_bottleneck_end_value\",        default=0.,     type=float,     help=\"End value of the l2_bottleneck schedule\")\n",
    "parser.add_argument(\"--l2_bottleneck_decay_power\",      default=0.5,    type=float,     help=\"Power of the Polynomial schedule\")\n",
    "parser.add_argument(\"--l2_bottleneck_decay_steps\",      default=1000,   type=int,       help=\"Number of steps until end of schedule is reached\")\n",
    "parser.add_argument(\"--staircase\",                      action=\"store_true\",            help=\"Learning rate schedule only change after decay steps if enabled.\")\n",
    "parser.add_argument(\"--clipping\",                       action=\"store_true\",            help=\"Clip backprop gradients between -10 and 10.\")\n",
    "parser.add_argument(\"--patience\",                       default=np.inf, type=int,       help=\"Number of step at which training is stopped if no improvement is recorder.\")\n",
    "parser.add_argument(\"--tolerance\",                      default=0,      type=float,     help=\"Current score <= (1 - tolerance) * best score => reset patience, else reduce patience.\")\n",
    "parser.add_argument(\"--track_train\",                    action=\"store_true\",            help=\"Track training metric instead of validation metric, in case we want to overfit\")\n",
    "parser.add_argument(\"--max_time\",                       default=np.inf, type=float,     help=\"Time allowed for the training, in hours.\")\n",
    "\n",
    "# logs\n",
    "parser.add_argument(\"--logdir\",                  default=\"None\",                help=\"Path of logs directory. Default if None, no logs recorded.\")\n",
    "parser.add_argument(\"--logname\",                 default=None,                  help=\"Overwrite name of the log with this argument\")\n",
    "parser.add_argument(\"--logname_prefixe\",         default=\"KappaVAE\",            help=\"If name of the log is not provided, this prefix is prepended to the date\")\n",
    "parser.add_argument(\"--model_dir\",               default=\"None\",                help=\"Path to the directory where to save models checkpoints.\")\n",
    "parser.add_argument(\"--checkpoints\",             default=10,    type=int,       help=\"Save a checkpoint of the models each {%} iteration.\")\n",
    "parser.add_argument(\"--max_to_keep\",             default=3,     type=int,       help=\"Max model checkpoint to keep.\")\n",
    "parser.add_argument(\"--n_residuals\",             default=5,     type=int,       help=\"Number of residual plots to save. Add overhead at the end of an epoch only.\")\n",
    "\n",
    "# Reproducibility params\n",
    "parser.add_argument(\"--seed\",                   default=None,   type=int,       help=\"Random seed for numpy and tensorflow.\")\n",
    "parser.add_argument(\"--json_override\",          default=None,   nargs=\"+\",      help=\"A json filepath that will override every command line parameters. Useful for reproducibility\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129d3224-90ac-42ab-8972-227920501581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 | train loss 1.517e+06 | val loss 2.424e+02 | learning rate 1.00e-04 | time per step 1.70e-01 s | beta 1.00 | skip strength 3.16e-04 | l2_bottleneck 9.54e-03\n",
      "epoch 1 | train loss 2.212e+02 | val loss 1.779e+02 | learning rate 9.00e-05 | time per step 1.67e-01 s | beta 1.00 | skip strength 0.00e+00 | l2_bottleneck 9.06e-03\n",
      "epoch 2 | train loss 1.707e+02 | val loss 1.547e+02 | learning rate 8.10e-05 | time per step 1.64e-01 s | beta 1.00 | skip strength 0.00e+00 | l2_bottleneck 8.54e-03\n",
      "epoch 3 | train loss 1.460e+02 | val loss 1.318e+02 | learning rate 7.29e-05 | time per step 1.64e-01 s | beta 1.00 | skip strength 0.00e+00 | l2_bottleneck 8.00e-03\n",
      "epoch 4 | train loss 1.293e+02 | val loss 1.226e+02 | learning rate 6.56e-05 | time per step 1.63e-01 s | beta 1.00 | skip strength 0.00e+00 | l2_bottleneck 7.42e-03\n",
      "epoch 5 | train loss 1.194e+02 | val loss 1.408e+02 | learning rate 5.90e-05 | time per step 1.66e-01 s | beta 1.00 | skip strength 0.00e+00 | l2_bottleneck 6.78e-03\n",
      "epoch 6 | train loss 1.137e+02 | val loss 1.121e+02 | learning rate 5.31e-05 | time per step 1.66e-01 s | beta 1.00 | skip strength 0.00e+00 | l2_bottleneck 6.08e-03\n",
      "epoch 7 | train loss 1.103e+02 | val loss 1.073e+02 | learning rate 4.78e-05 | time per step 1.64e-01 s | beta 1.00 | skip strength 0.00e+00 | l2_bottleneck 5.29e-03\n",
      "epoch 8 | train loss 1.060e+02 | val loss 1.059e+02 | learning rate 4.30e-05 | time per step 1.64e-01 s | beta 1.00 | skip strength 0.00e+00 | l2_bottleneck 4.36e-03\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.argv = ['']\n",
    "args = parser.parse_args(\n",
    "  f\"--datasets {os.getenv('CENSAI_PATH')}/data/hkappa128_TNG100_trainset/ \"\\\n",
    "  f\"--compression_type=GZIP \"\\\n",
    "  f\"--epochs=200 \"\\\n",
    "  f\"--optimizer=Adam \"\\\n",
    "  f\"--initial_learning_rate 1e-4 \"\\\n",
    "  f\"--decay_rate=0.9 \"\\\n",
    "  f\"--decay_steps=1000 \"\\\n",
    "  f\"--beta_init=1 \"\\\n",
    "  f\"--beta_end_value=1 \"\\\n",
    "  f\"--beta_decay_power=1. \"\\\n",
    "  f\"--beta_decay_steps=5000 \"\\\n",
    "  f\"--beta_cyclical=0 \"\\\n",
    "  f\"--skip_strength_init=1e-3 \"\\\n",
    "  f\"--skip_strength_end_value=0. \"\\\n",
    "  f\"--skip_strength_decay_power=0.5 \"\\\n",
    "  f\"--skip_strength_decay_steps=1000 \"\\\n",
    "  f\"--l2_bottleneck_init=1e-2 \"\\\n",
    "  f\"--l2_bottleneck_end_value=0. \"\\\n",
    "  f\"--l2_bottleneck_decay_power=0.5 \"\\\n",
    "  f\"--l2_bottleneck_decay_steps=10000 \"\\\n",
    "  f\"--staircase \"\\\n",
    "  f\"--clipping \"\\\n",
    "  f\"--patience=20 \"\\\n",
    "  f\"--tolerance=0.01 \"\\\n",
    "  f\"--batch_size=20 \"\\\n",
    "  f\"--train_split=0.9 \"\\\n",
    "  f\"--total_items=20000 \"\\\n",
    "  f\"--block_length=1 \"\\\n",
    "  f\"--layers=4 \"\\\n",
    "  f\"--res_blocks_in_layer=8 \"\\\n",
    "  f\"--conv_layers_per_block=2 \"\\\n",
    "  f\"--filter_scaling=2 \"\\\n",
    "  f\"--filters=16 \"\\\n",
    "  f\"--kernel_size=3 \"\\\n",
    "  f\"--res_architecture=full_pre_activation \"\\\n",
    "  f\"--kernel_reg_amp=1e-4 \"\\\n",
    "  f\"--bias_reg_amp=1e-4 \"\\\n",
    "  f\"--activation=leaky_relu \"\\\n",
    "  f\"--batch_norm=1 \"\\\n",
    "  f\"--latent_size=512 \"\\\n",
    "  f\"--cache_file={os.getenv('SLURM_TMPDIR')}/cache \"\\\n",
    "#   f\"--logdir={os.getenv('CENSAI_PATH')}/logsRVAE_k \"\\\n",
    "  f\"--logname_prefixe=RVAE1_TNG100 \"\\\n",
    "#   f\"--model_dir={os.getenv('CENSAI_PATH')}/models \"\\\n",
    "  f\"--checkpoints=5 \"\\\n",
    "  f\"--max_to_keep=3 \"\\\n",
    "  f\"--max_time=0.5 \".split()\n",
    ")\n",
    "\n",
    "cache_files = glob.glob(f\"{os.getenv('SLURM_TMPDIR')}/cache*\")\n",
    "for cache in cache_files:\n",
    "    os.remove(cache)\n",
    "if args.seed is not None:\n",
    "    tf.random.set_seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "history, vae, train_dataset, val_dataset = main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8df4fa-f063-4be2-96dd-72dd81db44ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = 1\n",
    "for batch, x in enumerate(train_dataset):\n",
    "    if batch == b:\n",
    "        for res_idx in range(args.batch_size):\n",
    "            kappa_true = x[res_idx, ...]\n",
    "            kappa_pred = vae(kappa_true[None, ...])[0, ...]\n",
    "            fig = residual_plot(kappa_true, kappa_pred)\n",
    "            break\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1c1a30-aa13-4274-81f0-5a797cb8fbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = tf.random.normal([9, vae.latent_size//2], dtype=tf.float32)\n",
    "y = vae.decode(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14dea11f-4557-49a4-84c6-142e63c3ba00",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3, 3, figsize=(12, 12))\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        k = 3 * j + i \n",
    "        axs[i, j].imshow(y[k, ..., 0], cmap=\"hot\", origin=\"lower\")\n",
    "        axs[i, j].axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac36755-311d-4714-80b6-969f7568dcd7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "censai",
   "language": "python",
   "name": "censai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
