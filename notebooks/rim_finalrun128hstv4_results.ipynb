{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "infectious-stock",
   "metadata": {},
   "outputs": [],
   "source": [
    "from censai import RIMSharedUnetv3, PhysicalModelv2, PowerSpectrum, RIMSourceUnetv2, AnalyticalPhysicalModel\n",
    "from censai.models import SharedUnetModelv4, UnetModelv2\n",
    "from censai.data.lenses_tng_v3 import decode_results, decode_physical_model_info, decode_all\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os, glob, json\n",
    "import h5py\n",
    "from censai.definitions import log_10\n",
    "from tqdm import tqdm\n",
    "from argparse import Namespace\n",
    "\n",
    "\n",
    "result_dir = os.path.join(os.getenv(\"CENSAI_PATH\"), \"results\")\n",
    "data_path = os.path.join(os.getenv(\"CENSAI_PATH\"), \"data\")\n",
    "models_path = os.path.join(os.getenv(\"CENSAI_PATH\"), \"models\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "tamil-waste",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"RIMSU128hstv4_control_008_RMSP1_TS8_NLtanh_TWuniform_211117121747\"\n",
    "source_model = \"RIMSource128hstv3_control_001_A0_L2_FLM0.0_211108220845\"\n",
    "_train_dataset = \"lenses128hst_TNG_rau_200k_control_denoised_validated_train\"\n",
    "_val_dataset = \"lenses128hst_TNG_rau_200k_control_denoised_validated_val\"\n",
    "_test_dataset = \"lenses128hst_TNG_rau_200k_control_denoised_testset_validated\"\n",
    "bins=40\n",
    "\n",
    "checkpoints_dir = os.path.join(os.getenv(\"CENSAI_PATH\"), \"models\", model)\n",
    "\n",
    "with open(os.path.join(checkpoints_dir, \"script_params.json\"), \"r\") as f:\n",
    "    args = json.load(f)\n",
    "args = Namespace(**args)\n",
    "\n",
    "files = glob.glob(os.path.join(os.getenv('CENSAI_PATH'), \"data\", _train_dataset, \"*.tfrecords\"))\n",
    "files = tf.data.Dataset.from_tensor_slices(files)\n",
    "train_dataset = files.interleave(\n",
    "    lambda x: tf.data.TFRecordDataset(x, compression_type=args.compression_type).shuffle(len(files)), block_length=1, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "# Read off global parameters from first example in dataset\n",
    "for physical_params in train_dataset.map(decode_physical_model_info):\n",
    "    break\n",
    "train_dataset = train_dataset.map(decode_results).shuffle(buffer_size=args.buffer_size)\n",
    "\n",
    "files = glob.glob(os.path.join(os.getenv('CENSAI_PATH'), \"data\", _val_dataset, \"*.tfrecords\"))\n",
    "files = tf.data.Dataset.from_tensor_slices(files)\n",
    "val_dataset = files.interleave(\n",
    "    lambda x: tf.data.TFRecordDataset(x, compression_type=args.compression_type).shuffle(len(files)), block_length=1, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "val_dataset = val_dataset.map(decode_results).shuffle(buffer_size=args.buffer_size)\n",
    "\n",
    "files = glob.glob(os.path.join(os.getenv('CENSAI_PATH'), \"data\", _test_dataset, \"*.tfrecords\"))\n",
    "files = tf.data.Dataset.from_tensor_slices(files)\n",
    "test_dataset = files.interleave(\n",
    "    lambda x: tf.data.TFRecordDataset(x, compression_type=args.compression_type).shuffle(len(files)), block_length=1, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.map(decode_results).shuffle(buffer_size=args.buffer_size)\n",
    "\n",
    "ps_lens = PowerSpectrum(bins=bins, pixels=physical_params[\"pixels\"].numpy())\n",
    "ps_source = PowerSpectrum(bins=bins,  pixels=physical_params[\"src pixels\"].numpy())\n",
    "ps_kappa = PowerSpectrum(bins=bins,  pixels=physical_params[\"kappa pixels\"].numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "happy-surge",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x2b5068ae62b0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phys = PhysicalModelv2(\n",
    "    pixels=physical_params[\"pixels\"].numpy(),\n",
    "    kappa_pixels=physical_params[\"kappa pixels\"].numpy(),\n",
    "    src_pixels=physical_params[\"src pixels\"].numpy(),\n",
    "    image_fov=physical_params[\"image fov\"].numpy(),\n",
    "    kappa_fov=physical_params[\"kappa fov\"].numpy(),\n",
    "    src_fov=physical_params[\"source fov\"].numpy(),\n",
    "    method=\"fft\",\n",
    ")\n",
    "\n",
    "phys_sie = AnalyticalPhysicalModel(\n",
    "    pixels=physical_params[\"pixels\"].numpy(),\n",
    "    image_fov=physical_params[\"image fov\"].numpy(),\n",
    "    src_fov=physical_params[\"source fov\"].numpy()\n",
    ")\n",
    "\n",
    "# Load RIM for source only\n",
    "rim_source_dir = os.path.join(os.getenv('CENSAI_PATH'), \"models\", source_model)\n",
    "with open(os.path.join(rim_source_dir, \"unet_hparams.json\")) as f:\n",
    "    unet_source_params = json.load(f)\n",
    "unet_source = UnetModelv2(**unet_source_params)\n",
    "with open(os.path.join(rim_source_dir, \"rim_hparams.json\")) as f:\n",
    "    rim_source_params = json.load(f)\n",
    "rim_source = RIMSourceUnetv2(phys, unet_source, **rim_source_params)\n",
    "ckpt_s = tf.train.Checkpoint(net=unet_source)\n",
    "checkpoint_manager_s = tf.train.CheckpointManager(ckpt_s, rim_source_dir, 1)\n",
    "checkpoint_manager_s.checkpoint.restore(checkpoint_manager_s.latest_checkpoint).expect_partial()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95061e07-e4d9-4f8b-96a0-e3f8f318b6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(models_path, model, \"unet_hparams.json\")) as f:\n",
    "    unet_params = json.load(f)\n",
    "\n",
    "unet = SharedUnetModelv4(**unet_params)\n",
    "ckpt = tf.train.Checkpoint(net=unet)\n",
    "checkpoint_manager = tf.train.CheckpointManager(ckpt, os.path.join(models_path, model), 1)\n",
    "checkpoint_manager.checkpoint.restore(checkpoint_manager.latest_checkpoint).expect_partial()\n",
    "with open(os.path.join(models_path, model, \"rim_hparams.json\")) as f:\n",
    "    rim_params = json.load(f)\n",
    "\n",
    "rim = RIMSharedUnetv3(phys, unet, **rim_params)\n",
    "\n",
    "train_size = 1000\n",
    "val_size = 1000\n",
    "test_size = 1000\n",
    "sie_size = 1000\n",
    "dataset_names = [_train_dataset, _val_dataset, _test_dataset]\n",
    "dataset_shapes = [train_size, val_size, test_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "466a87a5-bbab-4738-a716-13a45b43a681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sie params\n",
    "max_shift = 0.1\n",
    "max_theta_e = 2.5\n",
    "min_theta_e = 0.5\n",
    "max_ellipticity = 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "human-agenda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "1000it [12:21,  1.35it/s]\n"
     ]
    }
   ],
   "source": [
    "output_file = os.path.join(os.getenv(\"CENSAI_PATH\"), \"results\", model + \"_\" + source_model + \"_temp.h5\")\n",
    "with h5py.File(output_file, 'w') as hf:\n",
    "    for i, dataset in tqdm(enumerate([train_dataset, val_dataset, test_dataset])):\n",
    "#     for i, dataset in tqdm(enumerate([])):\n",
    "        g = hf.create_group(f'{dataset_names[i]}')\n",
    "        data_len = dataset_shapes[i]\n",
    "        g.create_dataset(name=\"lens\", shape=[data_len, phys.pixels, phys.pixels, 1], dtype=np.float32)\n",
    "        g.create_dataset(name=\"psf\",  shape=[data_len, physical_params['psf pixels'], physical_params['psf pixels'], 1], dtype=np.float32)\n",
    "        g.create_dataset(name=\"psf_fwhm\", shape=[data_len], dtype=np.float32)\n",
    "        g.create_dataset(name=\"noise_rms\", shape=[data_len], dtype=np.float32)\n",
    "        g.create_dataset(name=\"source\", shape=[data_len, phys.src_pixels, phys.src_pixels, 1], dtype=np.float32)\n",
    "        g.create_dataset(name=\"kappa\", shape=[data_len, phys.kappa_pixels, phys.kappa_pixels, 1], dtype=np.float32)\n",
    "        g.create_dataset(name=\"lens_pred\", shape=[data_len, phys.pixels, phys.pixels, 1], dtype=np.float32)\n",
    "        g.create_dataset(name=\"lens_pred2\", shape=[data_len, phys.pixels, phys.pixels, 1], dtype=np.float32)\n",
    "        g.create_dataset(name=\"source_pred\", shape=[data_len, rim.steps, phys.src_pixels, phys.src_pixels, 1], dtype=np.float32)\n",
    "        g.create_dataset(name=\"source_pred2\", shape=[data_len, rim.steps, phys.src_pixels, phys.src_pixels, 1], dtype=np.float32)\n",
    "        g.create_dataset(name=\"kappa_pred\", shape=[data_len, rim.steps, phys.kappa_pixels, phys.kappa_pixels, 1], dtype=np.float32)\n",
    "        g.create_dataset(name=\"chi_squared\", shape=[data_len, rim.steps], dtype=np.float32)\n",
    "        g.create_dataset(name=\"chi_squared2\", shape=[data_len, rim.steps], dtype=np.float32)\n",
    "        g.create_dataset(name=\"lens_coherence_spectrum\", shape=[data_len, bins], dtype=np.float32)\n",
    "        g.create_dataset(name=\"source_coherence_spectrum\",  shape=[data_len, bins], dtype=np.float32)\n",
    "        g.create_dataset(name=\"lens_coherence_spectrum2\", shape=[data_len, bins], dtype=np.float32)\n",
    "        g.create_dataset(name=\"source_coherence_spectrum2\",  shape=[data_len, bins], dtype=np.float32)\n",
    "        g.create_dataset(name=\"kappa_coherence_spectrum\", shape=[data_len, bins], dtype=np.float32)\n",
    "        g.create_dataset(name=\"lens_frequencies\", shape=[bins], dtype=np.float32)\n",
    "        g.create_dataset(name=\"source_frequencies\", shape=[bins], dtype=np.float32)\n",
    "        g.create_dataset(name=\"kappa_frequencies\", shape=[bins], dtype=np.float32)\n",
    "        g.create_dataset(name=\"kappa_fov\", shape=[1], dtype=np.float32)\n",
    "        g.create_dataset(name=\"source_fov\", shape=[1], dtype=np.float32)\n",
    "        g.create_dataset(name=\"lens_fov\", shape=[1], dtype=np.float32)\n",
    "\n",
    "        for batch, (lens, source, kappa, noise_rms, psf, fwhm) in tqdm(enumerate(dataset.take(data_len).batch(args.batch_size).prefetch(tf.data.experimental.AUTOTUNE))):\n",
    "            batch_size = lens.shape[0]\n",
    "            # Compute predictions for kappa and source\n",
    "            source_pred, kappa_pred, chi_squared = rim.predict(lens, noise_rms, psf)\n",
    "            lens_pred = phys.forward(source_pred[-1], kappa_pred[-1], psf)\n",
    "            # Re-optimize source with a trained source model\n",
    "            source_pred2, chi_squared2 = rim_source.predict(lens, kappa_pred[-1], noise_rms, psf)\n",
    "            lens_pred2 = phys.forward(source_pred2[-1], kappa_pred[-1], psf)\n",
    "            # Compute Power spectrum of converged predictions\n",
    "            _ps_lens = ps_lens.cross_correlation_coefficient(lens[..., 0], lens_pred[..., 0])\n",
    "            _ps_lens2 = ps_lens.cross_correlation_coefficient(lens[..., 0], lens_pred2[..., 0])\n",
    "            _ps_kappa = ps_kappa.cross_correlation_coefficient(log_10(kappa)[..., 0], log_10(kappa_pred[-1])[..., 0])\n",
    "            _ps_source = ps_source.cross_correlation_coefficient(source[..., 0], source_pred[-1][..., 0])\n",
    "            _ps_source2 = ps_source.cross_correlation_coefficient(source[..., 0], source_pred2[-1][..., 0])\n",
    "\n",
    "            # save results\n",
    "            i_begin = batch * args.batch_size\n",
    "            i_end = i_begin + batch_size\n",
    "            g[\"lens\"][i_begin:i_end] = lens.numpy().astype(np.float32)\n",
    "            g[\"psf\"][i_begin:i_end] = psf.numpy().astype(np.float32)\n",
    "            g[\"psf_fwhm\"][i_begin:i_end] = fwhm.numpy().astype(np.float32)\n",
    "            g[\"noise_rms\"][i_begin:i_end] = noise_rms.numpy().astype(np.float32)\n",
    "            g[\"source\"][i_begin:i_end] = source.numpy().astype(np.float32)\n",
    "            g[\"kappa\"][i_begin:i_end] = kappa.numpy().astype(np.float32)\n",
    "            g[\"lens_pred\"][i_begin:i_end] = lens_pred.numpy().astype(np.float32)\n",
    "            g[\"lens_pred2\"][i_begin:i_end] = lens_pred2.numpy().astype(np.float32)\n",
    "            g[\"source_pred\"][i_begin:i_end] = tf.transpose(source_pred, perm=(1, 0, 2, 3, 4)).numpy().astype(np.float32)\n",
    "            g[\"source_pred2\"][i_begin:i_end] = tf.transpose(source_pred2, perm=(1, 0, 2, 3, 4)).numpy().astype(np.float32)\n",
    "            g[\"kappa_pred\"][i_begin:i_end] = tf.transpose(kappa_pred, perm=(1, 0, 2, 3, 4)).numpy().astype(np.float32)\n",
    "            g[\"chi_squared\"][i_begin:i_end] = tf.transpose(chi_squared).numpy().astype(np.float32)\n",
    "            g[\"chi_squared2\"][i_begin:i_end] = tf.transpose(chi_squared2).numpy().astype(np.float32)\n",
    "            g[\"lens_coherence_spectrum\"][i_begin:i_end] = _ps_lens\n",
    "            g[\"lens_coherence_spectrum2\"][i_begin:i_end] = _ps_lens2\n",
    "            g[\"source_coherence_spectrum\"][i_begin:i_end] = _ps_source\n",
    "            g[\"source_coherence_spectrum2\"][i_begin:i_end] = _ps_source2\n",
    "            g[\"lens_coherence_spectrum\"][i_begin:i_end] = _ps_lens\n",
    "            g[\"lens_coherence_spectrum\"][i_begin:i_end] = _ps_lens\n",
    "            g[\"kappa_coherence_spectrum\"][i_begin:i_end] = _ps_kappa\n",
    "\n",
    "            if batch == 0:\n",
    "                _, f = np.histogram(np.fft.fftfreq(phys.pixels)[:phys.pixels//2], bins=ps_lens.bins)\n",
    "                f = (f[:-1] + f[1:]) / 2\n",
    "                g[\"lens_frequencies\"][:] = f\n",
    "                _, f = np.histogram(np.fft.fftfreq(phys.src_pixels)[:phys.src_pixels//2], bins=ps_source.bins)\n",
    "                f = (f[:-1] + f[1:]) / 2\n",
    "                g[\"source_frequencies\"][:] = f\n",
    "                _, f = np.histogram(np.fft.fftfreq(phys.kappa_pixels)[:phys.kappa_pixels//2], bins=ps_kappa.bins)\n",
    "                f = (f[:-1] + f[1:]) / 2\n",
    "                g[\"kappa_frequencies\"][:] = f\n",
    "                g[\"kappa_fov\"][0] = phys.kappa_fov\n",
    "                g[\"source_fov\"][0] = phys.src_fov\n",
    "\n",
    "    # Create SIE test\n",
    "#     g = hf.create_group(f'SIE_test')\n",
    "    g = hf[\"SIE_test\"]\n",
    "    data_len = sie_size\n",
    "    sie_dataset = test_dataset.take(data_len)\n",
    "    g.create_dataset(name=\"lens\", shape=[data_len, phys.pixels, phys.pixels, 1], dtype=np.float32)\n",
    "    g.create_dataset(name=\"psf\",  shape=[data_len, physical_params['psf pixels'], physical_params['psf pixels'], 1], dtype=np.float32)\n",
    "    g.create_dataset(name=\"psf_fwhm\", shape=[data_len], dtype=np.float32)\n",
    "    g.create_dataset(name=\"noise_rms\", shape=[data_len], dtype=np.float32)\n",
    "    g.create_dataset(name=\"source\", shape=[data_len, phys.src_pixels, phys.src_pixels, 1], dtype=np.float32)\n",
    "    g.create_dataset(name=\"kappa\", shape=[data_len, phys.kappa_pixels, phys.kappa_pixels, 1], dtype=np.float32)\n",
    "    g.create_dataset(name=\"lens_pred\", shape=[data_len, phys.pixels, phys.pixels, 1], dtype=np.float32)\n",
    "    g.create_dataset(name=\"lens_pred2\", shape=[data_len, phys.pixels, phys.pixels, 1], dtype=np.float32)\n",
    "    g.create_dataset(name=\"source_pred\", shape=[data_len, rim.steps, phys.src_pixels, phys.src_pixels, 1], dtype=np.float32)\n",
    "    g.create_dataset(name=\"source_pred2\", shape=[data_len, rim.steps, phys.src_pixels, phys.src_pixels, 1], dtype=np.float32)\n",
    "    g.create_dataset(name=\"kappa_pred\", shape=[data_len, rim.steps, phys.kappa_pixels, phys.kappa_pixels, 1], dtype=np.float32)\n",
    "    g.create_dataset(name=\"chi_squared\", shape=[data_len, rim.steps], dtype=np.float32)\n",
    "    g.create_dataset(name=\"chi_squared2\", shape=[data_len, rim.steps], dtype=np.float32)\n",
    "    g.create_dataset(name=\"lens_coherence_spectrum\", shape=[data_len, bins], dtype=np.float32)\n",
    "    g.create_dataset(name=\"source_coherence_spectrum\",  shape=[data_len, bins], dtype=np.float32)\n",
    "    g.create_dataset(name=\"lens_coherence_spectrum2\", shape=[data_len, bins], dtype=np.float32)\n",
    "    g.create_dataset(name=\"source_coherence_spectrum2\",  shape=[data_len, bins], dtype=np.float32)\n",
    "    g.create_dataset(name=\"kappa_coherence_spectrum\", shape=[data_len,bins], dtype=np.float32)\n",
    "    g.create_dataset(name=\"lens_frequencies\", shape=[bins], dtype=np.float32)\n",
    "    g.create_dataset(name=\"source_frequencies\", shape=[bins], dtype=np.float32)\n",
    "    g.create_dataset(name=\"kappa_frequencies\", shape=[bins], dtype=np.float32)\n",
    "    g.create_dataset(name=\"einstein_radius\", shape=[data_len], dtype=np.float32)\n",
    "    g.create_dataset(name=\"position\", shape=[data_len, 2], dtype=np.float32)\n",
    "    g.create_dataset(name=\"orientation\", shape=[data_len], dtype=np.float32)\n",
    "    g.create_dataset(name=\"ellipticity\", shape=[data_len], dtype=np.float32)\n",
    "    g.create_dataset(name=\"kappa_fov\", shape=[1], dtype=np.float32)\n",
    "    g.create_dataset(name=\"source_fov\", shape=[1], dtype=np.float32)\n",
    "    g.create_dataset(name=\"lens_fov\", shape=[1], dtype=np.float32)\n",
    "\n",
    "    for batch, (_, source, _, noise_rms, psf, fwhm) in tqdm(enumerate(sie_dataset.take(data_len).batch(1).prefetch(tf.data.experimental.AUTOTUNE))):\n",
    "        batch_size = source.shape[0]\n",
    "        # Create some SIE kappa maps\n",
    "        _r = tf.random.uniform(shape=[batch_size, 1, 1, 1], minval=0, maxval=max_shift)\n",
    "        _theta = tf.random.uniform(shape=[batch_size, 1, 1, 1], minval=-np.pi, maxval=np.pi)\n",
    "        x0 = _r * tf.math.cos(_theta)\n",
    "        y0 = _r * tf.math.sin(_theta)\n",
    "        ellipticity = tf.random.uniform(shape=[batch_size, 1, 1, 1], minval=0., maxval=max_ellipticity)\n",
    "        phi = tf.random.uniform(shape=[batch_size, 1, 1, 1], minval=-np.pi, maxval=np.pi)\n",
    "        einstein_radius = tf.random.uniform(shape=[batch_size, 1, 1, 1], minval=min_theta_e, maxval=max_theta_e)\n",
    "        kappa = phys_sie.kappa_field(x0, y0, ellipticity, phi, einstein_radius)\n",
    "        lens = phys.noisy_forward(source, kappa, noise_rms=noise_rms, psf=psf)\n",
    "\n",
    "        # Compute predictions for kappa and source\n",
    "        source_pred, kappa_pred, chi_squared = rim.predict(lens, noise_rms, psf)\n",
    "        lens_pred = phys.forward(source_pred[-1], kappa_pred[-1], psf)\n",
    "        # Re-optimize source with a trained source model\n",
    "        source_pred2, chi_squared2 = rim_source.predict(lens, kappa_pred[-1], noise_rms, psf)\n",
    "        lens_pred2 = phys.forward(source_pred2[-1], kappa_pred[-1], psf)\n",
    "        # Compute Power spectrum of converged predictions\n",
    "        _ps_lens = ps_lens.cross_correlation_coefficient(lens[..., 0], lens_pred[..., 0])\n",
    "        _ps_lens2 = ps_lens.cross_correlation_coefficient(lens[..., 0], lens_pred2[..., 0])\n",
    "        _ps_kappa = ps_kappa.cross_correlation_coefficient(log_10(kappa)[..., 0], log_10(kappa_pred[-1])[..., 0])\n",
    "        _ps_source = ps_source.cross_correlation_coefficient(source[..., 0], source_pred[-1][..., 0])\n",
    "        _ps_source2 = ps_source.cross_correlation_coefficient(source[..., 0], source_pred2[-1][..., 0])\n",
    "\n",
    "        # save results\n",
    "        i_begin = batch * args.batch_size\n",
    "        i_end = i_begin + batch_size\n",
    "        g[\"lens\"][i_begin:i_end] = lens.numpy().astype(np.float32)\n",
    "        g[\"psf\"][i_begin:i_end] = psf.numpy().astype(np.float32)\n",
    "        g[\"psf_fwhm\"][i_begin:i_end] = fwhm.numpy().astype(np.float32)\n",
    "        g[\"noise_rms\"][i_begin:i_end] = noise_rms.numpy().astype(np.float32)\n",
    "        g[\"source\"][i_begin:i_end] = source.numpy().astype(np.float32)\n",
    "        g[\"kappa\"][i_begin:i_end] = kappa.numpy().astype(np.float32)\n",
    "        g[\"lens_pred\"][i_begin:i_end] = lens_pred.numpy().astype(np.float32)\n",
    "        g[\"lens_pred2\"][i_begin:i_end] = lens_pred2.numpy().astype(np.float32)\n",
    "        g[\"source_pred\"][i_begin:i_end] = tf.transpose(source_pred, perm=(1, 0, 2, 3, 4)).numpy().astype(np.float32)\n",
    "        g[\"source_pred2\"][i_begin:i_end] = tf.transpose(source_pred2, perm=(1, 0, 2, 3, 4)).numpy().astype(np.float32)\n",
    "        g[\"kappa_pred\"][i_begin:i_end] = tf.transpose(kappa_pred, perm=(1, 0, 2, 3, 4)).numpy().astype(np.float32)\n",
    "        g[\"chi_squared\"][i_begin:i_end] = tf.transpose(chi_squared).numpy().astype(np.float32)\n",
    "        g[\"chi_squared2\"][i_begin:i_end] = tf.transpose(chi_squared2).numpy().astype(np.float32)\n",
    "        g[\"lens_coherence_spectrum\"][i_begin:i_end] = _ps_lens.numpy().astype(np.float32)\n",
    "        g[\"lens_coherence_spectrum2\"][i_begin:i_end] = _ps_lens2.numpy().astype(np.float32)\n",
    "        g[\"source_coherence_spectrum\"][i_begin:i_end] = _ps_source.numpy().astype(np.float32)\n",
    "        g[\"source_coherence_spectrum2\"][i_begin:i_end] = _ps_source2.numpy().astype(np.float32)\n",
    "        g[\"kappa_coherence_spectrum\"][i_begin:i_end] = _ps_kappa.numpy().astype(np.float32)\n",
    "        g[\"einstein_radius\"][i_begin:i_end] = einstein_radius[:, 0, 0, 0].numpy().astype(np.float32)\n",
    "        g[\"position\"][i_begin:i_end] = tf.stack([x0[:, 0, 0, 0], y0[:, 0, 0, 0]], axis=1).numpy().astype(np.float32)\n",
    "        g[\"ellipticity\"][i_begin:i_end] = ellipticity[:, 0, 0, 0].numpy().astype(np.float32)\n",
    "        g[\"orientation\"][i_begin:i_end] = phi[:, 0, 0, 0].numpy().astype(np.float32)\n",
    "\n",
    "        if batch == 0:\n",
    "            _, f = np.histogram(np.fft.fftfreq(phys.pixels)[:phys.pixels // 2], bins=ps_lens.bins)\n",
    "            f = (f[:-1] + f[1:]) / 2\n",
    "            g[\"lens_frequencies\"][:] = f\n",
    "            _, f = np.histogram(np.fft.fftfreq(phys.src_pixels)[:phys.src_pixels // 2], bins=ps_source.bins)\n",
    "            f = (f[:-1] + f[1:]) / 2\n",
    "            g[\"source_frequencies\"][:] = f\n",
    "            _, f = np.histogram(np.fft.fftfreq(phys.kappa_pixels)[:phys.kappa_pixels // 2], bins=ps_kappa.bins)\n",
    "            f = (f[:-1] + f[1:]) / 2\n",
    "            g[\"kappa_frequencies\"][:] = f\n",
    "            g[\"kappa_fov\"][0] = phys.kappa_fov\n",
    "            g[\"source_fov\"][0] = phys.src_fov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fancy-evans",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/aadam/scratch/Censai/results/RIMSU128hstv4_control_008_RMSP1_TS8_NLtanh_TWuniform_211117121747_RIMSource128hstv3_control_001_A0_L2_FLM0.0_211108220845_temp.h5'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pleasant-saturn",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "censai",
   "language": "python",
   "name": "censai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
